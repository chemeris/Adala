{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Quickstart","text":"<p>Adala is an Autonomous DAta (Labeling) Agent framework.</p> <p>Adala offers a robust framework for implementing agents specialized in data processing, with an emphasis on diverse data labeling tasks. These agents are autonomous, meaning they can independently acquire one or more skills through iterative learning. This learning process is influenced by their operating environment, observations, and reflections. Users define the environment by providing a ground truth dataset. Every agent learns and applies its skills in what we refer to as a \"runtime\", synonymous with LLM.</p> <p></p>"},{"location":"#installation","title":"Installation","text":"<p>Install Adala:</p> <pre><code>pip install adala\n</code></pre>"},{"location":"#prerequisites","title":"Prerequisites","text":"<p>Set OPENAI_API_KEY (see instructions here)</p>"},{"location":"#quickstart_1","title":"\ud83c\udfac Quickstart","text":"<p>In this example we will use Adala as a standalone library directly inside Python notebook.</p> <p>Click here to see an extended quickstart example. </p> <pre><code>import pandas as pd\n\nfrom adala.agents import Agent\nfrom adala.environments import StaticEnvironment\nfrom adala.skills import ClassificationSkill\nfrom adala.runtimes import OpenAIChatRuntime\nfrom rich import print\n\n# Train dataset\ntrain_df = pd.DataFrame([\n    [\"It was the negative first impressions, and then it started working.\", \"Positive\"],\n    [\"Not loud enough and doesn't turn on like it should.\", \"Negative\"],\n    [\"I don't know what to say.\", \"Neutral\"],\n    [\"Manager was rude, but the most important that mic shows very flat frequency response.\", \"Positive\"],\n    [\"The phone doesn't seem to accept anything except CBR mp3s.\", \"Negative\"],\n    [\"I tried it before, I bought this device for my son.\", \"Neutral\"],\n], columns=[\"text\", \"sentiment\"])\n\n# Test dataset\ntest_df = pd.DataFrame([\n    \"All three broke within two months of use.\",\n    \"The device worked for a long time, can't say anything bad.\",\n    \"Just a random line of text.\"\n], columns=[\"text\"])\n\nagent = Agent(\n    # connect to a dataset\n    environment=StaticEnvironment(df=train_df),\n\n    # define a skill\n    skills=ClassificationSkill(\n        name='sentiment',\n        instructions=\"Label text as positive, negative or neutral.\",\n        labels={'sentiment': [\"Positive\", \"Negative\", \"Neutral\"]},\n        input_template=\"Text: {text}\",\n        output_template=\"Sentiment: {sentiment}\"\n    ),\n\n    # define all the different runtimes your skills may use\n    runtimes = {\n        # You can specify your OPENAI API KEY here via `OpenAIRuntime(..., api_key='your-api-key')`\n        'openai': OpenAIChatRuntime(model='gpt-3.5-turbo'),\n    },\n    default_runtime='openai',\n\n    # NOTE! If you have access to GPT-4, you can uncomment the lines bellow for better results\n#     default_teacher_runtime='openai-gpt4',\n#     teacher_runtimes = {\n#       'openai-gpt4': OpenAIRuntime(model='gpt-4')\n#     }\n)\n\nprint(agent)\nprint(agent.skills)\n\nagent.learn(learning_iterations=3, accuracy_threshold=0.95)\n\nprint('\\n=&gt; Run tests ...')\npredictions = agent.run(test_df)\nprint('\\n =&gt; Test results:')\nprint(predictions)\n</code></pre>"},{"location":"#reference","title":"Reference","text":"<ul> <li>Agents - main interface for  interacting with environment</li> <li>Datasets - data inputs for agents</li> <li>Environments - environments for agents, where it collects ground truth signal</li> <li>Memories - agent's memory for storing and retrieving data</li> <li>Runtimes - agent's execution runtime (e.g. LLMs providers)</li> <li>Skills - agent skills for data labeling</li> </ul>"},{"location":"agents/","title":"Agents","text":""},{"location":"agents/#adala.agents.base.Agent","title":"<code>Agent</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Represents a customizable agent that can interact with environments, employ skills, and leverage memory and runtimes.</p> <p>Attributes:</p> Name Type Description <code>environment</code> <code>Environment</code> <p>The environment with which the agent interacts.</p> <code>skills</code> <code>Union[SkillSet, List[Skill]]</code> <p>The skills possessed by the agent.</p> <code>memory</code> <code>LongTermMemory</code> <p>The agent's long-term memory. Defaults to None.</p> <code>runtimes</code> <code>Dict[str, Runtime]</code> <p>The runtimes available to the agent. Defaults to predefined runtimes.</p> <code>default_runtime</code> <code>str</code> <p>The default runtime used by the agent. Defaults to 'openai'.</p> <code>teacher_runtimes</code> <code>Dict[str, Runtime]</code> <p>The runtimes available to the agent's teacher. Defaults to predefined runtimes.</p> <code>default_teacher_runtime</code> <code>str</code> <p>The default runtime used by the agent's teacher. Defaults to 'openai-gpt3'.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from adala.environments import StaticEnvironment\n&gt;&gt;&gt; from adala.skills import LinearSkillSet, TransformSkill\n&gt;&gt;&gt; from adala.agents import Agent\n&gt;&gt;&gt; agent = Agent(skills=LinearSkillSet(skills=[TransformSkill()]), environment=StaticEnvironment())\n&gt;&gt;&gt; agent.learn()  # starts the learning process\n&gt;&gt;&gt; predictions = agent.run()  # runs the agent and returns the predictions\n</code></pre> Source code in <code>adala/agents/base.py</code> <pre><code>class Agent(BaseModel, ABC):\n    \"\"\"\n    Represents a customizable agent that can interact with environments,\n    employ skills, and leverage memory and runtimes.\n\n    Attributes:\n        environment (Environment): The environment with which the agent interacts.\n        skills (Union[SkillSet, List[Skill]]): The skills possessed by the agent.\n        memory (LongTermMemory, optional): The agent's long-term memory. Defaults to None.\n        runtimes (Dict[str, Runtime], optional): The runtimes available to the agent. Defaults to predefined runtimes.\n        default_runtime (str): The default runtime used by the agent. Defaults to 'openai'.\n        teacher_runtimes (Dict[str, Runtime], optional): The runtimes available to the agent's teacher. Defaults to predefined runtimes.\n        default_teacher_runtime (str): The default runtime used by the agent's teacher. Defaults to 'openai-gpt3'.\n\n    Examples:\n        &gt;&gt;&gt; from adala.environments import StaticEnvironment\n        &gt;&gt;&gt; from adala.skills import LinearSkillSet, TransformSkill\n        &gt;&gt;&gt; from adala.agents import Agent\n        &gt;&gt;&gt; agent = Agent(skills=LinearSkillSet(skills=[TransformSkill()]), environment=StaticEnvironment())\n        &gt;&gt;&gt; agent.learn()  # starts the learning process\n        &gt;&gt;&gt; predictions = agent.run()  # runs the agent and returns the predictions\n    \"\"\"\n\n    environment: Optional[SerializeAsAny[Union[Environment, AsyncEnvironment]]] = None\n    skills: Union[Skill, SkillSet]\n\n    memory: Memory = Field(default=None)\n    runtimes: Dict[str, SerializeAsAny[Union[Runtime, AsyncRuntime]]] = Field(\n        default_factory=lambda: {\"default\": OpenAIChatRuntime(model=\"gpt-3.5-turbo\")}\n    )\n    default_runtime: str = \"default\"\n    teacher_runtimes: Dict[str, SerializeAsAny[Runtime]] = Field(\n        default_factory=lambda: {\"default\": None}\n    )\n    default_teacher_runtime: str = \"default\"\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    def __rich__(self) -&gt; str:\n        \"\"\"\n        Returns a colorized and formatted representation of the Agent instance.\n\n        Returns:\n            str: A rich-formatted representation of the agent.\n        \"\"\"\n\n        skill_names = \", \".join([skill.name for skill in self.skills.skills.values()])\n        runtime_names = \", \".join(self.runtimes.keys())\n\n        return (\n            f\"[bold blue]Agent Instance[/bold blue]\\n\\n\"\n            f\"Environment: {self.environment.__class__.__name__}\\n\"\n            f\"Skills: {skill_names}\\n\"\n            f\"Runtimes: {runtime_names}\\n\"\n            f\"Default Runtime: {self.default_runtime}\\n\"\n            f\"Default Teacher Runtime: {self.default_teacher_runtime}\"\n        )\n\n    @field_validator(\"environment\", mode=\"before\")\n    def environment_validator(cls, v) -&gt; Environment:\n        \"\"\"\n        Validates and possibly transforms the environment attribute:\n        if the environment is an InternalDataFrame, it is transformed into a StaticEnvironment.\n        \"\"\"\n        logger.debug(f\"Validating environment attribute: {v}\")\n        if isinstance(v, InternalDataFrame):\n            v = StaticEnvironment(df=v)\n        elif isinstance(v, dict) and \"type\" in v:\n            v = Environment.create_from_registry(v.pop(\"type\"), **v)\n        return v\n\n    @field_validator(\"skills\", mode=\"before\")\n    def skills_validator(cls, v) -&gt; SkillSet:\n        \"\"\"\n        Validates and possibly transforms the skills attribute.\n        \"\"\"\n        if isinstance(v, SkillSet):\n            return v\n        elif isinstance(v, Skill):\n            return LinearSkillSet(skills=[v])\n        elif isinstance(v, list):\n            return LinearSkillSet(skills=v)\n        else:\n            raise ValueError(\n                f\"skills must be of type SkillSet or Skill, but received type {type(v)}\"\n            )\n\n    @field_validator(\"runtimes\", mode=\"before\")\n    def runtimes_validator(cls, v) -&gt; Dict[str, Union[Runtime, AsyncRuntime]]:\n        \"\"\"\n        Validates and creates runtimes\n        \"\"\"\n        out = {}\n        for runtime_name, runtime_value in v.items():\n            if isinstance(runtime_value, dict):\n                if \"type\" not in runtime_value:\n                    raise ValueError(\n                        f\"Runtime {runtime_name} must have a 'type' field to specify the runtime type.\"\n                    )\n                type_name = runtime_value.pop(\"type\")\n                runtime_value = Runtime.create_from_registry(\n                    type=type_name, **runtime_value\n                )\n            out[runtime_name] = runtime_value\n        return out\n\n    @model_validator(mode=\"after\")\n    def verify_input_parameters(self):\n        \"\"\"\n        Verifies that the input parameters are valid.\"\"\"\n\n        def _raise_default_runtime_error(val, runtime, runtimes, default_value):\n            print_error(\n                f\"The Agent.{runtime} is set to {val}, \"\n                f\"but this runtime is not available in the list: {list(runtimes)}. \"\n                f\"Please choose one of the available runtimes and initialize the agent again, for example:\\n\\n\"\n                f\"agent = Agent(..., {runtime}='{default_value}')\\n\\n\"\n                f\"Make sure the default runtime is available in the list of runtimes. For example:\\n\\n\"\n                f\"agent = Agent(..., runtimes={{'{default_value}': OpenAIRuntime(model='gpt-4')}})\\n\\n\"\n            )\n            raise ValueError(f\"default runtime {val} not found in provided runtimes.\")\n\n        if self.default_runtime not in self.runtimes:\n            _raise_default_runtime_error(\n                self.default_runtime, \"default_runtime\", self.runtimes, \"openai\"\n            )\n        if self.default_teacher_runtime not in self.teacher_runtimes:\n            _raise_default_runtime_error(\n                self.default_teacher_runtime,\n                \"default_teacher_runtime\",\n                self.teacher_runtimes,\n                \"openai-gpt4\",\n            )\n        return self\n\n    def get_runtime(self, runtime: Optional[str] = None) -&gt; Runtime:\n        \"\"\"\n        Retrieves the specified runtime or the default runtime if none is specified.\n\n        Args:\n            runtime (str, optional): The name of the runtime to retrieve. Defaults to None.\n\n        Returns:\n            Runtime: The requested runtime.\n\n        Raises:\n            ValueError: If the specified runtime is not found.\n        \"\"\"\n\n        if runtime is None:\n            runtime = self.default_runtime\n        if runtime not in self.runtimes:\n            raise ValueError(f'Runtime \"{runtime}\" not found.')\n        return self.runtimes[runtime]\n\n    def get_teacher_runtime(self, runtime: Optional[str] = None) -&gt; Runtime:\n        \"\"\"\n        Retrieves the specified teacher runtime or the default runtime if none is specified.\n\n        Args:\n            runtime (str, optional): The name of the runtime to retrieve. Defaults to None.\n\n        Returns:\n            Runtime: The requested runtime.\n\n        Raises:\n            ValueError: If the specified runtime is not found.\n        \"\"\"\n\n        if runtime is None:\n            runtime = self.default_teacher_runtime\n        if runtime not in self.teacher_runtimes:\n            raise ValueError(f'Teacher Runtime \"{runtime}\" not found.')\n        runtime = self.teacher_runtimes[runtime]\n        if not runtime:\n            raise ValueError(\n                f\"Teacher Runtime is requested, but it was not set.\"\n                f\"Please provide a teacher runtime in the agent's constructor explicitly:\"\n                f\"agent = Agent(..., teacher_runtimes={{'default': OpenAIChatRuntime(model='gpt-4')}})\"\n            )\n        return runtime\n\n    def run(\n        self, input: InternalDataFrame = None, runtime: Optional[str] = None\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Runs the agent on the specified dataset.\n\n        Args:\n            input (InternalDataFrame): The dataset to run the agent on.\n            runtime (str, optional): The name of the runtime to use. Defaults to None, use the default runtime.\n\n        Returns:\n            InternalDataFrame: The dataset with the agent's predictions.\n        \"\"\"\n        if input is None:\n            if self.environment is None:\n                raise ValueError(\"input is None and no environment is set.\")\n            input = self.environment.get_data_batch(None)\n        runtime = self.get_runtime(runtime=runtime)\n        predictions = self.skills.apply(input, runtime=runtime)\n        return predictions\n\n    async def arun(\n        self, input: InternalDataFrame = None, runtime: Optional[str] = None\n    ) -&gt; Optional[InternalDataFrame]:\n        \"\"\"\n        Runs the agent on the specified input asynchronously.\n        If no input is specified, the agent will run on the environment until it is exhausted.\n        If input is specified, the agent will run on the input, ignoring the connected genvironment.\n\n        Args:\n            input (InternalDataFrame): The dataset to run the agent on.\n            runtime (str, optional): The name of the runtime to use. Defaults to None, use the default runtime.\n\n        Returns:\n            InternalDataFrame: The dataset with the agent's predictions.\n        \"\"\"\n\n        runtime = self.get_runtime(runtime=runtime)\n        if not isinstance(runtime, AsyncRuntime):\n            raise ValueError(\n                \"When using asynchronous run with `agent.arun()`, the runtime must be an AsyncRuntime.\"\n            )\n        else:\n            print(f\"Using runtime {type(runtime)}\")\n\n        if not isinstance(self.environment, AsyncEnvironment):\n            raise ValueError(\n                \"When using asynchronous run with `agent.arun()`, the environment must be an AsyncEnvironment.\"\n            )\n        if input is None:\n            if self.environment is None:\n                raise ValueError(\"input is None and no environment is set.\")\n            # run on the environment until it is exhausted\n            while True:\n                try:\n                    data_batch = await self.environment.get_data_batch(\n                        batch_size=runtime.batch_size\n                    )\n                    if data_batch.empty:\n                        print_text(\"No more data in the environment. Exiting.\")\n                        break\n                except Exception as e:\n                    # TODO: environment should raise a specific exception + log error\n                    print_error(f\"Error getting data batch from environment: {e}\")\n                    break\n                predictions = await self.skills.aapply(data_batch, runtime=runtime)\n                await self.environment.set_predictions(predictions)\n\n        else:\n            # single run on the input data\n            predictions = await self.skills.aapply(input, runtime=runtime)\n            return predictions\n\n    def select_skill_to_train(\n        self, feedback: EnvironmentFeedback, accuracy_threshold: float\n    ) -&gt; Tuple[str, str, float]:\n        \"\"\"\n        Selects the skill to train based on the feedback signal.\n\n        Args:\n            feedback (Feedback): The feedback signal.\n            accuracy_threshold (float): The accuracy threshold to use for selecting the skill to train.\n\n        Returns:\n            str: The name of the skill to train.\n            str: The name of the skill output to train.\n            float: The accuracy score of the skill to train.\n\n        \"\"\"\n\n        # Use ground truth signal to find the skill to improve\n        # TODO: what if it is not possible to estimate accuracy per skill?\n        accuracy = feedback.get_accuracy()\n        train_skill_name, train_skill_output, acc_score = \"\", \"\", None\n        for skill_output, skill_name in self.skills.get_skill_outputs().items():\n            if skill_output in accuracy and accuracy[skill_output] &lt; accuracy_threshold:\n                train_skill_name, train_skill_output = skill_name, skill_output\n                acc_score = accuracy[skill_output]\n                break\n\n        return train_skill_name, train_skill_output, acc_score\n\n    def learn(\n        self,\n        learning_iterations: int = 3,\n        accuracy_threshold: float = 0.9,\n        update_memory: bool = True,\n        batch_size: Optional[int] = None,\n        num_feedbacks: Optional[int] = None,\n        runtime: Optional[str] = None,\n        teacher_runtime: Optional[str] = None,\n    ):\n        \"\"\"\n        Enables the agent to learn and improve its skills based on interactions with its environment.\n\n        Args:\n            learning_iterations (int, optional): The number of iterations for learning. Defaults to 3.\n            accuracy_threshold (float, optional): The desired accuracy threshold to reach. Defaults to 0.9.\n            update_memory (bool, optional): Flag to determine if memory should be updated after learning. Defaults to True.\n            num_feedbacks (int, optional): The number of predictions to request feedback for. Defaults to None.\n            runtime (str, optional): The runtime to be used for the learning process. Defaults to None.\n            teacher_runtime (str, optional): The teacher runtime to be used for the learning process. Defaults to None.\n        \"\"\"\n\n        runtime = self.get_runtime(runtime=runtime)\n        teacher_runtime = self.get_teacher_runtime(runtime=teacher_runtime)\n\n        for iteration in range(learning_iterations):\n            print_text(\n                f\"\\n\\n=&gt; Iteration #{iteration}: Getting feedback, analyzing and improving ...\"\n            )\n\n            inputs = self.environment.get_data_batch(batch_size=batch_size)\n            predictions = self.skills.apply(inputs, runtime=runtime)\n            feedback = self.environment.get_feedback(\n                self.skills, predictions, num_feedbacks=num_feedbacks\n            )\n            # TODO: this is just pretty printing - remove later for efficiency\n            print(\"Predictions and feedback:\")\n            print_dataframe(\n                feedback.feedback.rename(\n                    columns=lambda x: x + \"__fb\" if x in predictions.columns else x\n                ).merge(predictions, left_index=True, right_index=True)\n            )\n            # -----------------------------\n            skill_mismatch = feedback.match.fillna(True) == False\n            has_errors = skill_mismatch.any(axis=1).any()\n            if not has_errors:\n                print_text(\"No errors found!\")\n                continue\n            first_skill_with_errors = skill_mismatch.any(axis=0).idxmax()\n\n            accuracy = feedback.get_accuracy()\n            # TODO: iterating over skill can be more complex, and we should take order into account\n            for skill_output, skill_name in self.skills.get_skill_outputs().items():\n                skill = self.skills[skill_name]\n                if skill.frozen:\n                    continue\n\n                print_text(\n                    f'Skill output to improve: \"{skill_output}\" (Skill=\"{skill_name}\")\\n'\n                    f\"Accuracy = {accuracy[skill_output] * 100:0.2f}%\",\n                    style=\"bold red\",\n                )\n\n                old_instructions = skill.instructions\n                skill.improve(\n                    predictions, skill_output, feedback, runtime=teacher_runtime\n                )\n\n                if is_running_in_jupyter():\n                    highlight_differences(old_instructions, skill.instructions)\n                else:\n                    print_text(skill.instructions, style=\"bold green\")\n\n                if skill_name == first_skill_with_errors:\n                    break\n\n        print_text(\"Train is done!\")\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.__rich__","title":"<code>__rich__()</code>","text":"<p>Returns a colorized and formatted representation of the Agent instance.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A rich-formatted representation of the agent.</p> Source code in <code>adala/agents/base.py</code> <pre><code>def __rich__(self) -&gt; str:\n    \"\"\"\n    Returns a colorized and formatted representation of the Agent instance.\n\n    Returns:\n        str: A rich-formatted representation of the agent.\n    \"\"\"\n\n    skill_names = \", \".join([skill.name for skill in self.skills.skills.values()])\n    runtime_names = \", \".join(self.runtimes.keys())\n\n    return (\n        f\"[bold blue]Agent Instance[/bold blue]\\n\\n\"\n        f\"Environment: {self.environment.__class__.__name__}\\n\"\n        f\"Skills: {skill_names}\\n\"\n        f\"Runtimes: {runtime_names}\\n\"\n        f\"Default Runtime: {self.default_runtime}\\n\"\n        f\"Default Teacher Runtime: {self.default_teacher_runtime}\"\n    )\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.arun","title":"<code>arun(input=None, runtime=None)</code>  <code>async</code>","text":"<p>Runs the agent on the specified input asynchronously. If no input is specified, the agent will run on the environment until it is exhausted. If input is specified, the agent will run on the input, ignoring the connected genvironment.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>InternalDataFrame</code> <p>The dataset to run the agent on.</p> <code>None</code> <code>runtime</code> <code>str</code> <p>The name of the runtime to use. Defaults to None, use the default runtime.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>Optional[InternalDataFrame]</code> <p>The dataset with the agent's predictions.</p> Source code in <code>adala/agents/base.py</code> <pre><code>async def arun(\n    self, input: InternalDataFrame = None, runtime: Optional[str] = None\n) -&gt; Optional[InternalDataFrame]:\n    \"\"\"\n    Runs the agent on the specified input asynchronously.\n    If no input is specified, the agent will run on the environment until it is exhausted.\n    If input is specified, the agent will run on the input, ignoring the connected genvironment.\n\n    Args:\n        input (InternalDataFrame): The dataset to run the agent on.\n        runtime (str, optional): The name of the runtime to use. Defaults to None, use the default runtime.\n\n    Returns:\n        InternalDataFrame: The dataset with the agent's predictions.\n    \"\"\"\n\n    runtime = self.get_runtime(runtime=runtime)\n    if not isinstance(runtime, AsyncRuntime):\n        raise ValueError(\n            \"When using asynchronous run with `agent.arun()`, the runtime must be an AsyncRuntime.\"\n        )\n    else:\n        print(f\"Using runtime {type(runtime)}\")\n\n    if not isinstance(self.environment, AsyncEnvironment):\n        raise ValueError(\n            \"When using asynchronous run with `agent.arun()`, the environment must be an AsyncEnvironment.\"\n        )\n    if input is None:\n        if self.environment is None:\n            raise ValueError(\"input is None and no environment is set.\")\n        # run on the environment until it is exhausted\n        while True:\n            try:\n                data_batch = await self.environment.get_data_batch(\n                    batch_size=runtime.batch_size\n                )\n                if data_batch.empty:\n                    print_text(\"No more data in the environment. Exiting.\")\n                    break\n            except Exception as e:\n                # TODO: environment should raise a specific exception + log error\n                print_error(f\"Error getting data batch from environment: {e}\")\n                break\n            predictions = await self.skills.aapply(data_batch, runtime=runtime)\n            await self.environment.set_predictions(predictions)\n\n    else:\n        # single run on the input data\n        predictions = await self.skills.aapply(input, runtime=runtime)\n        return predictions\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.environment_validator","title":"<code>environment_validator(v)</code>","text":"<p>Validates and possibly transforms the environment attribute: if the environment is an InternalDataFrame, it is transformed into a StaticEnvironment.</p> Source code in <code>adala/agents/base.py</code> <pre><code>@field_validator(\"environment\", mode=\"before\")\ndef environment_validator(cls, v) -&gt; Environment:\n    \"\"\"\n    Validates and possibly transforms the environment attribute:\n    if the environment is an InternalDataFrame, it is transformed into a StaticEnvironment.\n    \"\"\"\n    logger.debug(f\"Validating environment attribute: {v}\")\n    if isinstance(v, InternalDataFrame):\n        v = StaticEnvironment(df=v)\n    elif isinstance(v, dict) and \"type\" in v:\n        v = Environment.create_from_registry(v.pop(\"type\"), **v)\n    return v\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.get_runtime","title":"<code>get_runtime(runtime=None)</code>","text":"<p>Retrieves the specified runtime or the default runtime if none is specified.</p> <p>Parameters:</p> Name Type Description Default <code>runtime</code> <code>str</code> <p>The name of the runtime to retrieve. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Runtime</code> <code>Runtime</code> <p>The requested runtime.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified runtime is not found.</p> Source code in <code>adala/agents/base.py</code> <pre><code>def get_runtime(self, runtime: Optional[str] = None) -&gt; Runtime:\n    \"\"\"\n    Retrieves the specified runtime or the default runtime if none is specified.\n\n    Args:\n        runtime (str, optional): The name of the runtime to retrieve. Defaults to None.\n\n    Returns:\n        Runtime: The requested runtime.\n\n    Raises:\n        ValueError: If the specified runtime is not found.\n    \"\"\"\n\n    if runtime is None:\n        runtime = self.default_runtime\n    if runtime not in self.runtimes:\n        raise ValueError(f'Runtime \"{runtime}\" not found.')\n    return self.runtimes[runtime]\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.get_teacher_runtime","title":"<code>get_teacher_runtime(runtime=None)</code>","text":"<p>Retrieves the specified teacher runtime or the default runtime if none is specified.</p> <p>Parameters:</p> Name Type Description Default <code>runtime</code> <code>str</code> <p>The name of the runtime to retrieve. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Runtime</code> <code>Runtime</code> <p>The requested runtime.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified runtime is not found.</p> Source code in <code>adala/agents/base.py</code> <pre><code>def get_teacher_runtime(self, runtime: Optional[str] = None) -&gt; Runtime:\n    \"\"\"\n    Retrieves the specified teacher runtime or the default runtime if none is specified.\n\n    Args:\n        runtime (str, optional): The name of the runtime to retrieve. Defaults to None.\n\n    Returns:\n        Runtime: The requested runtime.\n\n    Raises:\n        ValueError: If the specified runtime is not found.\n    \"\"\"\n\n    if runtime is None:\n        runtime = self.default_teacher_runtime\n    if runtime not in self.teacher_runtimes:\n        raise ValueError(f'Teacher Runtime \"{runtime}\" not found.')\n    runtime = self.teacher_runtimes[runtime]\n    if not runtime:\n        raise ValueError(\n            f\"Teacher Runtime is requested, but it was not set.\"\n            f\"Please provide a teacher runtime in the agent's constructor explicitly:\"\n            f\"agent = Agent(..., teacher_runtimes={{'default': OpenAIChatRuntime(model='gpt-4')}})\"\n        )\n    return runtime\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.learn","title":"<code>learn(learning_iterations=3, accuracy_threshold=0.9, update_memory=True, batch_size=None, num_feedbacks=None, runtime=None, teacher_runtime=None)</code>","text":"<p>Enables the agent to learn and improve its skills based on interactions with its environment.</p> <p>Parameters:</p> Name Type Description Default <code>learning_iterations</code> <code>int</code> <p>The number of iterations for learning. Defaults to 3.</p> <code>3</code> <code>accuracy_threshold</code> <code>float</code> <p>The desired accuracy threshold to reach. Defaults to 0.9.</p> <code>0.9</code> <code>update_memory</code> <code>bool</code> <p>Flag to determine if memory should be updated after learning. Defaults to True.</p> <code>True</code> <code>num_feedbacks</code> <code>int</code> <p>The number of predictions to request feedback for. Defaults to None.</p> <code>None</code> <code>runtime</code> <code>str</code> <p>The runtime to be used for the learning process. Defaults to None.</p> <code>None</code> <code>teacher_runtime</code> <code>str</code> <p>The teacher runtime to be used for the learning process. Defaults to None.</p> <code>None</code> Source code in <code>adala/agents/base.py</code> <pre><code>def learn(\n    self,\n    learning_iterations: int = 3,\n    accuracy_threshold: float = 0.9,\n    update_memory: bool = True,\n    batch_size: Optional[int] = None,\n    num_feedbacks: Optional[int] = None,\n    runtime: Optional[str] = None,\n    teacher_runtime: Optional[str] = None,\n):\n    \"\"\"\n    Enables the agent to learn and improve its skills based on interactions with its environment.\n\n    Args:\n        learning_iterations (int, optional): The number of iterations for learning. Defaults to 3.\n        accuracy_threshold (float, optional): The desired accuracy threshold to reach. Defaults to 0.9.\n        update_memory (bool, optional): Flag to determine if memory should be updated after learning. Defaults to True.\n        num_feedbacks (int, optional): The number of predictions to request feedback for. Defaults to None.\n        runtime (str, optional): The runtime to be used for the learning process. Defaults to None.\n        teacher_runtime (str, optional): The teacher runtime to be used for the learning process. Defaults to None.\n    \"\"\"\n\n    runtime = self.get_runtime(runtime=runtime)\n    teacher_runtime = self.get_teacher_runtime(runtime=teacher_runtime)\n\n    for iteration in range(learning_iterations):\n        print_text(\n            f\"\\n\\n=&gt; Iteration #{iteration}: Getting feedback, analyzing and improving ...\"\n        )\n\n        inputs = self.environment.get_data_batch(batch_size=batch_size)\n        predictions = self.skills.apply(inputs, runtime=runtime)\n        feedback = self.environment.get_feedback(\n            self.skills, predictions, num_feedbacks=num_feedbacks\n        )\n        # TODO: this is just pretty printing - remove later for efficiency\n        print(\"Predictions and feedback:\")\n        print_dataframe(\n            feedback.feedback.rename(\n                columns=lambda x: x + \"__fb\" if x in predictions.columns else x\n            ).merge(predictions, left_index=True, right_index=True)\n        )\n        # -----------------------------\n        skill_mismatch = feedback.match.fillna(True) == False\n        has_errors = skill_mismatch.any(axis=1).any()\n        if not has_errors:\n            print_text(\"No errors found!\")\n            continue\n        first_skill_with_errors = skill_mismatch.any(axis=0).idxmax()\n\n        accuracy = feedback.get_accuracy()\n        # TODO: iterating over skill can be more complex, and we should take order into account\n        for skill_output, skill_name in self.skills.get_skill_outputs().items():\n            skill = self.skills[skill_name]\n            if skill.frozen:\n                continue\n\n            print_text(\n                f'Skill output to improve: \"{skill_output}\" (Skill=\"{skill_name}\")\\n'\n                f\"Accuracy = {accuracy[skill_output] * 100:0.2f}%\",\n                style=\"bold red\",\n            )\n\n            old_instructions = skill.instructions\n            skill.improve(\n                predictions, skill_output, feedback, runtime=teacher_runtime\n            )\n\n            if is_running_in_jupyter():\n                highlight_differences(old_instructions, skill.instructions)\n            else:\n                print_text(skill.instructions, style=\"bold green\")\n\n            if skill_name == first_skill_with_errors:\n                break\n\n    print_text(\"Train is done!\")\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.run","title":"<code>run(input=None, runtime=None)</code>","text":"<p>Runs the agent on the specified dataset.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>InternalDataFrame</code> <p>The dataset to run the agent on.</p> <code>None</code> <code>runtime</code> <code>str</code> <p>The name of the runtime to use. Defaults to None, use the default runtime.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The dataset with the agent's predictions.</p> Source code in <code>adala/agents/base.py</code> <pre><code>def run(\n    self, input: InternalDataFrame = None, runtime: Optional[str] = None\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Runs the agent on the specified dataset.\n\n    Args:\n        input (InternalDataFrame): The dataset to run the agent on.\n        runtime (str, optional): The name of the runtime to use. Defaults to None, use the default runtime.\n\n    Returns:\n        InternalDataFrame: The dataset with the agent's predictions.\n    \"\"\"\n    if input is None:\n        if self.environment is None:\n            raise ValueError(\"input is None and no environment is set.\")\n        input = self.environment.get_data_batch(None)\n    runtime = self.get_runtime(runtime=runtime)\n    predictions = self.skills.apply(input, runtime=runtime)\n    return predictions\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.runtimes_validator","title":"<code>runtimes_validator(v)</code>","text":"<p>Validates and creates runtimes</p> Source code in <code>adala/agents/base.py</code> <pre><code>@field_validator(\"runtimes\", mode=\"before\")\ndef runtimes_validator(cls, v) -&gt; Dict[str, Union[Runtime, AsyncRuntime]]:\n    \"\"\"\n    Validates and creates runtimes\n    \"\"\"\n    out = {}\n    for runtime_name, runtime_value in v.items():\n        if isinstance(runtime_value, dict):\n            if \"type\" not in runtime_value:\n                raise ValueError(\n                    f\"Runtime {runtime_name} must have a 'type' field to specify the runtime type.\"\n                )\n            type_name = runtime_value.pop(\"type\")\n            runtime_value = Runtime.create_from_registry(\n                type=type_name, **runtime_value\n            )\n        out[runtime_name] = runtime_value\n    return out\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.select_skill_to_train","title":"<code>select_skill_to_train(feedback, accuracy_threshold)</code>","text":"<p>Selects the skill to train based on the feedback signal.</p> <p>Parameters:</p> Name Type Description Default <code>feedback</code> <code>Feedback</code> <p>The feedback signal.</p> required <code>accuracy_threshold</code> <code>float</code> <p>The accuracy threshold to use for selecting the skill to train.</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The name of the skill to train.</p> <code>str</code> <code>str</code> <p>The name of the skill output to train.</p> <code>float</code> <code>float</code> <p>The accuracy score of the skill to train.</p> Source code in <code>adala/agents/base.py</code> <pre><code>def select_skill_to_train(\n    self, feedback: EnvironmentFeedback, accuracy_threshold: float\n) -&gt; Tuple[str, str, float]:\n    \"\"\"\n    Selects the skill to train based on the feedback signal.\n\n    Args:\n        feedback (Feedback): The feedback signal.\n        accuracy_threshold (float): The accuracy threshold to use for selecting the skill to train.\n\n    Returns:\n        str: The name of the skill to train.\n        str: The name of the skill output to train.\n        float: The accuracy score of the skill to train.\n\n    \"\"\"\n\n    # Use ground truth signal to find the skill to improve\n    # TODO: what if it is not possible to estimate accuracy per skill?\n    accuracy = feedback.get_accuracy()\n    train_skill_name, train_skill_output, acc_score = \"\", \"\", None\n    for skill_output, skill_name in self.skills.get_skill_outputs().items():\n        if skill_output in accuracy and accuracy[skill_output] &lt; accuracy_threshold:\n            train_skill_name, train_skill_output = skill_name, skill_output\n            acc_score = accuracy[skill_output]\n            break\n\n    return train_skill_name, train_skill_output, acc_score\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.skills_validator","title":"<code>skills_validator(v)</code>","text":"<p>Validates and possibly transforms the skills attribute.</p> Source code in <code>adala/agents/base.py</code> <pre><code>@field_validator(\"skills\", mode=\"before\")\ndef skills_validator(cls, v) -&gt; SkillSet:\n    \"\"\"\n    Validates and possibly transforms the skills attribute.\n    \"\"\"\n    if isinstance(v, SkillSet):\n        return v\n    elif isinstance(v, Skill):\n        return LinearSkillSet(skills=[v])\n    elif isinstance(v, list):\n        return LinearSkillSet(skills=v)\n    else:\n        raise ValueError(\n            f\"skills must be of type SkillSet or Skill, but received type {type(v)}\"\n        )\n</code></pre>"},{"location":"agents/#adala.agents.base.Agent.verify_input_parameters","title":"<code>verify_input_parameters()</code>","text":"<p>Verifies that the input parameters are valid.</p> Source code in <code>adala/agents/base.py</code> <pre><code>@model_validator(mode=\"after\")\ndef verify_input_parameters(self):\n    \"\"\"\n    Verifies that the input parameters are valid.\"\"\"\n\n    def _raise_default_runtime_error(val, runtime, runtimes, default_value):\n        print_error(\n            f\"The Agent.{runtime} is set to {val}, \"\n            f\"but this runtime is not available in the list: {list(runtimes)}. \"\n            f\"Please choose one of the available runtimes and initialize the agent again, for example:\\n\\n\"\n            f\"agent = Agent(..., {runtime}='{default_value}')\\n\\n\"\n            f\"Make sure the default runtime is available in the list of runtimes. For example:\\n\\n\"\n            f\"agent = Agent(..., runtimes={{'{default_value}': OpenAIRuntime(model='gpt-4')}})\\n\\n\"\n        )\n        raise ValueError(f\"default runtime {val} not found in provided runtimes.\")\n\n    if self.default_runtime not in self.runtimes:\n        _raise_default_runtime_error(\n            self.default_runtime, \"default_runtime\", self.runtimes, \"openai\"\n        )\n    if self.default_teacher_runtime not in self.teacher_runtimes:\n        _raise_default_runtime_error(\n            self.default_teacher_runtime,\n            \"default_teacher_runtime\",\n            self.teacher_runtimes,\n            \"openai-gpt4\",\n        )\n    return self\n</code></pre>"},{"location":"agents/#adala.agents.base.create_agent_from_dict","title":"<code>create_agent_from_dict(json_dict)</code>","text":"<p>Creates an agent from a JSON dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>json_dict</code> <code>Dict</code> <p>The JSON dictionary to create the agent from.</p> required <p>Returns:</p> Name Type Description <code>Agent</code> <p>The created agent.</p> Source code in <code>adala/agents/base.py</code> <pre><code>def create_agent_from_dict(json_dict: Dict):\n    \"\"\"\n    Creates an agent from a JSON dictionary.\n\n    Args:\n        json_dict (Dict): The JSON dictionary to create the agent from.\n\n    Returns:\n        Agent: The created agent.\n    \"\"\"\n\n    agent = Agent(**json_dict)\n    return agent\n</code></pre>"},{"location":"agents/#adala.agents.base.create_agent_from_file","title":"<code>create_agent_from_file(file_path)</code>","text":"<p>Creates an agent from a YAML file: 1. Define agent reasoning workflow in <code>workflow.yml</code>:</p> <pre><code>- name: reasoning\n  type: sample_transform\n  sample_size: 10\n  instructions: \"Think step-by-step.\"\n  input_template: \"Question: {question}\"\n  output_template: \"{reasoning}\"\n\n- name: numeric_answer\n  type: transform\n  instructions: &gt;\n    Given math question and reasoning, provide only numeric answer after `Answer: `, for example:\n    Question: &lt;math question&gt;\n    Reasoning: &lt;reasoning&gt;\n    Answer: &lt;your numerical answer&gt;\n  input_template: &gt;\n    Question: {question}\n    Reasoning: {reasoning}\n  output_template: &gt;\n    Answer: {answer}\n</code></pre> <ol> <li>Run adala math reasoning workflow on the <code>gsm8k</code> dataset:</li> </ol> <pre><code>adala run --input gsm8k --dataset-config main --dataset-split test --workflow workflow.yml\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the YAML file to create the agent from.</p> required <p>Returns:</p> Name Type Description <code>Agent</code> <p>The created agent.</p> Source code in <code>adala/agents/base.py</code> <pre><code>def create_agent_from_file(file_path: str):\n    \"\"\"\n    Creates an agent from a YAML file:\n    1. Define agent reasoning workflow in `workflow.yml`:\n\n    ```yaml\n    - name: reasoning\n      type: sample_transform\n      sample_size: 10\n      instructions: \"Think step-by-step.\"\n      input_template: \"Question: {question}\"\n      output_template: \"{reasoning}\"\n\n    - name: numeric_answer\n      type: transform\n      instructions: &gt;\n        Given math question and reasoning, provide only numeric answer after `Answer: `, for example:\n        Question: &lt;math question&gt;\n        Reasoning: &lt;reasoning&gt;\n        Answer: &lt;your numerical answer&gt;\n      input_template: &gt;\n        Question: {question}\n        Reasoning: {reasoning}\n      output_template: &gt;\n        Answer: {answer}\n    ```\n\n    2. Run adala math reasoning workflow on the `gsm8k` dataset:\n\n    ```sh\n    adala run --input gsm8k --dataset-config main --dataset-split test --workflow workflow.yml\n    ```\n\n    Args:\n        file_path (str): The path to the YAML file to create the agent from.\n\n    Returns:\n        Agent: The created agent.\n    \"\"\"\n\n    with open(file_path, \"r\") as file:\n        json_dict = yaml.safe_load(file)\n    if isinstance(json_dict, list):\n        json_dict = {\"skills\": json_dict}\n    return create_agent_from_dict(json_dict)\n</code></pre>"},{"location":"environments/","title":"Environments","text":""},{"location":"environments/#adala.environments.base.AsyncEnvironment","title":"<code>AsyncEnvironment</code>","text":"<p>             Bases: <code>Environment</code>, <code>ABC</code></p> Source code in <code>adala/environments/base.py</code> <pre><code>class AsyncEnvironment(Environment, ABC):\n    @abstractmethod\n    async def initialize(self):\n        \"\"\"\n        Initialize the environment, e.g by connecting to a database, reading file to memory or starting a stream.\n\n        Raises:\n            NotImplementedError: This method is not implemented for BasicEnvironment.\n        \"\"\"\n\n    @abstractmethod\n    async def finalize(self):\n        \"\"\"\n        Finalize the environment, e.g by closing a database connection, closing a file or stopping a stream.\n\n        Raises:\n            NotImplementedError: This method is not implemented for BasicEnvironment.\n        \"\"\"\n\n    @abstractmethod\n    async def get_data_batch(self, batch_size: Optional[int]) -&gt; InternalDataFrame:\n        \"\"\"\n        Get a batch of data from data stream to be processed by the skill set.\n\n        Args:\n            batch_size (Optional[int], optional): The size of the batch. Defaults to None\n\n        Returns:\n            InternalDataFrame: The data batch.\n        \"\"\"\n\n    @abstractmethod\n    async def get_feedback(\n        self,\n        skills: SkillSet,\n        predictions: InternalDataFrame,\n        num_feedbacks: Optional[int] = None,\n    ) -&gt; EnvironmentFeedback:\n        \"\"\"\n        Request feedback for the predictions.\n\n        Args:\n            skills (SkillSet): The set of skills/models whose predictions are being evaluated.\n            predictions (InternalDataFrame): The predictions to compare with the ground truth.\n            num_feedbacks (Optional[int], optional): The number of feedbacks to request. Defaults to all predictions\n        Returns:\n            EnvironmentFeedback: The resulting ground truth signal, with matches and errors detailed.\n        \"\"\"\n\n    @abstractmethod\n    async def set_predictions(self, predictions: InternalDataFrame):\n        \"\"\"\n        Push predictions back to the environment.\n\n        Args:\n            predictions (InternalDataFrame): The predictions to push to the environment.\n        \"\"\"\n\n    @abstractmethod\n    async def save(self):\n        \"\"\"\n        Save the current state of the BasicEnvironment.\n\n        Raises:\n            NotImplementedError: This method is not implemented for BasicEnvironment.\n        \"\"\"\n\n    @abstractmethod\n    async def restore(self):\n        \"\"\"\n        Restore the state of the BasicEnvironment.\n\n        Raises:\n            NotImplementedError: This method is not implemented for BasicEnvironment.\n        \"\"\"\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"environments/#adala.environments.base.AsyncEnvironment.finalize","title":"<code>finalize()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Finalize the environment, e.g by closing a database connection, closing a file or stopping a stream.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method is not implemented for BasicEnvironment.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\nasync def finalize(self):\n    \"\"\"\n    Finalize the environment, e.g by closing a database connection, closing a file or stopping a stream.\n\n    Raises:\n        NotImplementedError: This method is not implemented for BasicEnvironment.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.AsyncEnvironment.get_data_batch","title":"<code>get_data_batch(batch_size)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Get a batch of data from data stream to be processed by the skill set.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>Optional[int]</code> <p>The size of the batch. Defaults to None</p> required <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The data batch.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\nasync def get_data_batch(self, batch_size: Optional[int]) -&gt; InternalDataFrame:\n    \"\"\"\n    Get a batch of data from data stream to be processed by the skill set.\n\n    Args:\n        batch_size (Optional[int], optional): The size of the batch. Defaults to None\n\n    Returns:\n        InternalDataFrame: The data batch.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.AsyncEnvironment.get_feedback","title":"<code>get_feedback(skills, predictions, num_feedbacks=None)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Request feedback for the predictions.</p> <p>Parameters:</p> Name Type Description Default <code>skills</code> <code>SkillSet</code> <p>The set of skills/models whose predictions are being evaluated.</p> required <code>predictions</code> <code>InternalDataFrame</code> <p>The predictions to compare with the ground truth.</p> required <code>num_feedbacks</code> <code>Optional[int]</code> <p>The number of feedbacks to request. Defaults to all predictions</p> <code>None</code> <p>Returns:     EnvironmentFeedback: The resulting ground truth signal, with matches and errors detailed.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\nasync def get_feedback(\n    self,\n    skills: SkillSet,\n    predictions: InternalDataFrame,\n    num_feedbacks: Optional[int] = None,\n) -&gt; EnvironmentFeedback:\n    \"\"\"\n    Request feedback for the predictions.\n\n    Args:\n        skills (SkillSet): The set of skills/models whose predictions are being evaluated.\n        predictions (InternalDataFrame): The predictions to compare with the ground truth.\n        num_feedbacks (Optional[int], optional): The number of feedbacks to request. Defaults to all predictions\n    Returns:\n        EnvironmentFeedback: The resulting ground truth signal, with matches and errors detailed.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.AsyncEnvironment.initialize","title":"<code>initialize()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Initialize the environment, e.g by connecting to a database, reading file to memory or starting a stream.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method is not implemented for BasicEnvironment.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\nasync def initialize(self):\n    \"\"\"\n    Initialize the environment, e.g by connecting to a database, reading file to memory or starting a stream.\n\n    Raises:\n        NotImplementedError: This method is not implemented for BasicEnvironment.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.AsyncEnvironment.restore","title":"<code>restore()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Restore the state of the BasicEnvironment.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method is not implemented for BasicEnvironment.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\nasync def restore(self):\n    \"\"\"\n    Restore the state of the BasicEnvironment.\n\n    Raises:\n        NotImplementedError: This method is not implemented for BasicEnvironment.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.AsyncEnvironment.save","title":"<code>save()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Save the current state of the BasicEnvironment.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method is not implemented for BasicEnvironment.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\nasync def save(self):\n    \"\"\"\n    Save the current state of the BasicEnvironment.\n\n    Raises:\n        NotImplementedError: This method is not implemented for BasicEnvironment.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.AsyncEnvironment.set_predictions","title":"<code>set_predictions(predictions)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Push predictions back to the environment.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>InternalDataFrame</code> <p>The predictions to push to the environment.</p> required Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\nasync def set_predictions(self, predictions: InternalDataFrame):\n    \"\"\"\n    Push predictions back to the environment.\n\n    Args:\n        predictions (InternalDataFrame): The predictions to push to the environment.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.Environment","title":"<code>Environment</code>","text":"<p>             Bases: <code>BaseModelInRegistry</code></p> <p>An abstract base class that defines the structure and required methods for an environment in which machine learning models operate and are evaluated against ground truth data.</p> <p>Subclasses should implement methods to handle feedback requests, comparison to ground truth, dataset conversion, and state persistence.</p> Source code in <code>adala/environments/base.py</code> <pre><code>class Environment(BaseModelInRegistry):\n    \"\"\"\n    An abstract base class that defines the structure and required methods for an environment\n    in which machine learning models operate and are evaluated against ground truth data.\n\n    Subclasses should implement methods to handle feedback requests, comparison to ground truth,\n    dataset conversion, and state persistence.\n    \"\"\"\n\n    @abstractmethod\n    def initialize(self):\n        \"\"\"\n        Initialize the environment, e.g by connecting to a database, reading file to memory or starting a stream.\n\n        Raises:\n            NotImplementedError: This method is not implemented for BasicEnvironment.\n        \"\"\"\n\n    @abstractmethod\n    def finalize(self):\n        \"\"\"\n        Finalize the environment, e.g by closing a database connection, writing memory to file or stopping a stream.\n\n        Raises:\n            NotImplementedError: This method is not implemented for BasicEnvironment.\n        \"\"\"\n\n    @abstractmethod\n    def get_data_batch(self, batch_size: Optional[int]) -&gt; InternalDataFrame:\n        \"\"\"\n        Get a batch of data from data stream to be processed by the skill set.\n\n        Args:\n            batch_size (Optional[int], optional): The size of the batch. Defaults to None\n\n        Returns:\n            InternalDataFrame: The data batch.\n        \"\"\"\n\n    @abstractmethod\n    def get_feedback(\n        self,\n        skills: SkillSet,\n        predictions: InternalDataFrame,\n        num_feedbacks: Optional[int] = None,\n    ) -&gt; EnvironmentFeedback:\n        \"\"\"\n        Request feedback for the predictions.\n\n        Args:\n            skills (SkillSet): The set of skills/models whose predictions are being evaluated.\n            predictions (InternalDataFrame): The predictions to compare with the ground truth.\n            num_feedbacks (Optional[int], optional): The number of feedbacks to request. Defaults to all predictions\n        Returns:\n            EnvironmentFeedback: The resulting ground truth signal, with matches and errors detailed.\n        \"\"\"\n\n    @abstractmethod\n    def save(self):\n        \"\"\"\n        Save the current state of the BasicEnvironment.\n\n        Raises:\n            NotImplementedError: This method is not implemented for BasicEnvironment.\n        \"\"\"\n\n    @abstractmethod\n    def restore(self):\n        \"\"\"\n        Restore the state of the BasicEnvironment.\n\n        Raises:\n            NotImplementedError: This method is not implemented for BasicEnvironment.\n        \"\"\"\n\n    class Config:\n        arbitrary_types_allowed = True\n</code></pre>"},{"location":"environments/#adala.environments.base.Environment.finalize","title":"<code>finalize()</code>  <code>abstractmethod</code>","text":"<p>Finalize the environment, e.g by closing a database connection, writing memory to file or stopping a stream.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method is not implemented for BasicEnvironment.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\ndef finalize(self):\n    \"\"\"\n    Finalize the environment, e.g by closing a database connection, writing memory to file or stopping a stream.\n\n    Raises:\n        NotImplementedError: This method is not implemented for BasicEnvironment.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.Environment.get_data_batch","title":"<code>get_data_batch(batch_size)</code>  <code>abstractmethod</code>","text":"<p>Get a batch of data from data stream to be processed by the skill set.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>Optional[int]</code> <p>The size of the batch. Defaults to None</p> required <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The data batch.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\ndef get_data_batch(self, batch_size: Optional[int]) -&gt; InternalDataFrame:\n    \"\"\"\n    Get a batch of data from data stream to be processed by the skill set.\n\n    Args:\n        batch_size (Optional[int], optional): The size of the batch. Defaults to None\n\n    Returns:\n        InternalDataFrame: The data batch.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.Environment.get_feedback","title":"<code>get_feedback(skills, predictions, num_feedbacks=None)</code>  <code>abstractmethod</code>","text":"<p>Request feedback for the predictions.</p> <p>Parameters:</p> Name Type Description Default <code>skills</code> <code>SkillSet</code> <p>The set of skills/models whose predictions are being evaluated.</p> required <code>predictions</code> <code>InternalDataFrame</code> <p>The predictions to compare with the ground truth.</p> required <code>num_feedbacks</code> <code>Optional[int]</code> <p>The number of feedbacks to request. Defaults to all predictions</p> <code>None</code> <p>Returns:     EnvironmentFeedback: The resulting ground truth signal, with matches and errors detailed.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\ndef get_feedback(\n    self,\n    skills: SkillSet,\n    predictions: InternalDataFrame,\n    num_feedbacks: Optional[int] = None,\n) -&gt; EnvironmentFeedback:\n    \"\"\"\n    Request feedback for the predictions.\n\n    Args:\n        skills (SkillSet): The set of skills/models whose predictions are being evaluated.\n        predictions (InternalDataFrame): The predictions to compare with the ground truth.\n        num_feedbacks (Optional[int], optional): The number of feedbacks to request. Defaults to all predictions\n    Returns:\n        EnvironmentFeedback: The resulting ground truth signal, with matches and errors detailed.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.Environment.initialize","title":"<code>initialize()</code>  <code>abstractmethod</code>","text":"<p>Initialize the environment, e.g by connecting to a database, reading file to memory or starting a stream.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method is not implemented for BasicEnvironment.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\ndef initialize(self):\n    \"\"\"\n    Initialize the environment, e.g by connecting to a database, reading file to memory or starting a stream.\n\n    Raises:\n        NotImplementedError: This method is not implemented for BasicEnvironment.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.Environment.restore","title":"<code>restore()</code>  <code>abstractmethod</code>","text":"<p>Restore the state of the BasicEnvironment.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method is not implemented for BasicEnvironment.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\ndef restore(self):\n    \"\"\"\n    Restore the state of the BasicEnvironment.\n\n    Raises:\n        NotImplementedError: This method is not implemented for BasicEnvironment.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.Environment.save","title":"<code>save()</code>  <code>abstractmethod</code>","text":"<p>Save the current state of the BasicEnvironment.</p> <p>Raises:</p> Type Description <code>NotImplementedError</code> <p>This method is not implemented for BasicEnvironment.</p> Source code in <code>adala/environments/base.py</code> <pre><code>@abstractmethod\ndef save(self):\n    \"\"\"\n    Save the current state of the BasicEnvironment.\n\n    Raises:\n        NotImplementedError: This method is not implemented for BasicEnvironment.\n    \"\"\"\n</code></pre>"},{"location":"environments/#adala.environments.base.EnvironmentFeedback","title":"<code>EnvironmentFeedback</code>","text":"<p>             Bases: <code>BaseModel</code></p> <p>A class that represents the feedback received from an environment, along with the calculated correctness of predictions.</p> <p>Attributes:</p> Name Type Description <code>match</code> <code>InternalDataFrame</code> <p>A DataFrame indicating the correctness of predictions.                        Each row corresponds to a prediction, and each column is a boolean indicating if skill matches ground truth.                        Columns are named after the skill names.                        Indices correspond to prediction indices.                        Example:                            <code>| index | skill_1 | skill_2 | skill_3 |                             |-------|---------|---------|---------|                             | 0     | True    | True    | False   |                             | 1     | False   | False   | False   |                             | 2     | True    | True    | True    |</code></p> <code>feedback</code> <code>InternalDataFrame</code> <p>A DataFrame that contains ground truth feedback per each skill output</p> Source code in <code>adala/environments/base.py</code> <pre><code>class EnvironmentFeedback(BaseModel):\n    \"\"\"\n    A class that represents the feedback received from an environment,\n    along with the calculated correctness of predictions.\n\n    Attributes:\n        match (InternalDataFrame): A DataFrame indicating the correctness of predictions.\n                                   Each row corresponds to a prediction, and each column is a boolean indicating if skill matches ground truth.\n                                   Columns are named after the skill names.\n                                   Indices correspond to prediction indices.\n                                   Example:\n                                       ```\n                                        | index | skill_1 | skill_2 | skill_3 |\n                                        |-------|---------|---------|---------|\n                                        | 0     | True    | True    | False   |\n                                        | 1     | False   | False   | False   |\n                                        | 2     | True    | True    | True    |\n                                        ```\n        feedback (InternalDataFrame): A DataFrame that contains ground truth feedback per each skill output\n    \"\"\"\n\n    match: InternalDataFrame\n    feedback: InternalDataFrame\n\n    class Config:\n        arbitrary_types_allowed = True\n\n    def get_accuracy(self) -&gt; InternalSeries:\n        \"\"\"\n        Calculate the accuracy of predictions as the mean of matches.\n\n        Returns:\n            InternalSeries: A series representing the accuracy of predictions.\n        \"\"\"\n        return self.match.mean()\n\n    def __rich__(self):\n        text = \"[bold blue]Environment Feedback:[/bold blue]\\n\\n\"\n        text += f\"\\n[bold]Match[/bold]\\n{self.match}\"\n        if self.feedback is not None:\n            text += f\"\\n[bold]Feedback[/bold]\\n{self.feedback}\"\n        return text\n</code></pre>"},{"location":"environments/#adala.environments.base.EnvironmentFeedback.get_accuracy","title":"<code>get_accuracy()</code>","text":"<p>Calculate the accuracy of predictions as the mean of matches.</p> <p>Returns:</p> Name Type Description <code>InternalSeries</code> <code>InternalSeries</code> <p>A series representing the accuracy of predictions.</p> Source code in <code>adala/environments/base.py</code> <pre><code>def get_accuracy(self) -&gt; InternalSeries:\n    \"\"\"\n    Calculate the accuracy of predictions as the mean of matches.\n\n    Returns:\n        InternalSeries: A series representing the accuracy of predictions.\n    \"\"\"\n    return self.match.mean()\n</code></pre>"},{"location":"memories/","title":"Memories","text":""},{"location":"memories/#adala.memories.base.Memory","title":"<code>Memory</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Base class for memories.</p> Source code in <code>adala/memories/base.py</code> <pre><code>class Memory(BaseModel, ABC):\n\n    \"\"\"\n    Base class for memories.\n    \"\"\"\n\n    @abstractmethod\n    def remember(self, observation: str, data: Dict):\n        \"\"\"\n        Base method for remembering experiences in long term memory.\n        \"\"\"\n\n    def remember_many(self, observations: List[str], data: List[Dict]):\n        \"\"\"\n        Base method for remembering experiences in long term memory.\n        \"\"\"\n        for observation, d in zip(observations, data):\n            self.remember(observation, d)\n\n    @abstractmethod\n    def retrieve(self, observation: str, num_results: int = 1) -&gt; Any:\n        \"\"\"\n        Base method for retrieving past experiences from long term memory, based on current observations\n\n        Args:\n            observation: the current observation\n            num_results: the number of results to return\n        \"\"\"\n\n    def retrieve_many(self, observations: List[str], num_results: int = 1) -&gt; List[Any]:\n        \"\"\"\n        Base method for retrieving past experiences from long term memory, based on current observations\n\n        Args:\n            observation: the current observation\n            num_results: the number of results to return\n        \"\"\"\n        return [self.retrieve(observation) for observation in observations]\n\n    @abstractmethod\n    def clear(self):\n        \"\"\"\n        Base method for clearing memory.\n        \"\"\"\n</code></pre>"},{"location":"memories/#adala.memories.base.Memory.clear","title":"<code>clear()</code>  <code>abstractmethod</code>","text":"<p>Base method for clearing memory.</p> Source code in <code>adala/memories/base.py</code> <pre><code>@abstractmethod\ndef clear(self):\n    \"\"\"\n    Base method for clearing memory.\n    \"\"\"\n</code></pre>"},{"location":"memories/#adala.memories.base.Memory.remember","title":"<code>remember(observation, data)</code>  <code>abstractmethod</code>","text":"<p>Base method for remembering experiences in long term memory.</p> Source code in <code>adala/memories/base.py</code> <pre><code>@abstractmethod\ndef remember(self, observation: str, data: Dict):\n    \"\"\"\n    Base method for remembering experiences in long term memory.\n    \"\"\"\n</code></pre>"},{"location":"memories/#adala.memories.base.Memory.remember_many","title":"<code>remember_many(observations, data)</code>","text":"<p>Base method for remembering experiences in long term memory.</p> Source code in <code>adala/memories/base.py</code> <pre><code>def remember_many(self, observations: List[str], data: List[Dict]):\n    \"\"\"\n    Base method for remembering experiences in long term memory.\n    \"\"\"\n    for observation, d in zip(observations, data):\n        self.remember(observation, d)\n</code></pre>"},{"location":"memories/#adala.memories.base.Memory.retrieve","title":"<code>retrieve(observation, num_results=1)</code>  <code>abstractmethod</code>","text":"<p>Base method for retrieving past experiences from long term memory, based on current observations</p> <p>Parameters:</p> Name Type Description Default <code>observation</code> <code>str</code> <p>the current observation</p> required <code>num_results</code> <code>int</code> <p>the number of results to return</p> <code>1</code> Source code in <code>adala/memories/base.py</code> <pre><code>@abstractmethod\ndef retrieve(self, observation: str, num_results: int = 1) -&gt; Any:\n    \"\"\"\n    Base method for retrieving past experiences from long term memory, based on current observations\n\n    Args:\n        observation: the current observation\n        num_results: the number of results to return\n    \"\"\"\n</code></pre>"},{"location":"memories/#adala.memories.base.Memory.retrieve_many","title":"<code>retrieve_many(observations, num_results=1)</code>","text":"<p>Base method for retrieving past experiences from long term memory, based on current observations</p> <p>Parameters:</p> Name Type Description Default <code>observation</code> <p>the current observation</p> required <code>num_results</code> <code>int</code> <p>the number of results to return</p> <code>1</code> Source code in <code>adala/memories/base.py</code> <pre><code>def retrieve_many(self, observations: List[str], num_results: int = 1) -&gt; List[Any]:\n    \"\"\"\n    Base method for retrieving past experiences from long term memory, based on current observations\n\n    Args:\n        observation: the current observation\n        num_results: the number of results to return\n    \"\"\"\n    return [self.retrieve(observation) for observation in observations]\n</code></pre>"},{"location":"memories/#adala.memories.file_memory.FileMemory","title":"<code>FileMemory</code>","text":"<p>             Bases: <code>Memory</code></p> Source code in <code>adala/memories/file_memory.py</code> <pre><code>class FileMemory(Memory):\n    filepath: str\n\n    def remember(self, observation: str, experience: Any):\n        \"\"\"\n        Serialize experience in JSON and append to file\n        \"\"\"\n        with open(self.filepath) as f:\n            memory = json.load(f)\n        memory[observation] = experience\n        with open(self.filepath, \"w\") as f:\n            json.dump(memory, f, indent=2)\n\n    def retrieve(self, observation: str) -&gt; Any:\n        \"\"\"\n        Retrieve experience from file\n        \"\"\"\n        with open(self.filepath) as f:\n            memory = json.load(f)\n        return memory[observation]\n</code></pre>"},{"location":"memories/#adala.memories.file_memory.FileMemory.remember","title":"<code>remember(observation, experience)</code>","text":"<p>Serialize experience in JSON and append to file</p> Source code in <code>adala/memories/file_memory.py</code> <pre><code>def remember(self, observation: str, experience: Any):\n    \"\"\"\n    Serialize experience in JSON and append to file\n    \"\"\"\n    with open(self.filepath) as f:\n        memory = json.load(f)\n    memory[observation] = experience\n    with open(self.filepath, \"w\") as f:\n        json.dump(memory, f, indent=2)\n</code></pre>"},{"location":"memories/#adala.memories.file_memory.FileMemory.retrieve","title":"<code>retrieve(observation)</code>","text":"<p>Retrieve experience from file</p> Source code in <code>adala/memories/file_memory.py</code> <pre><code>def retrieve(self, observation: str) -&gt; Any:\n    \"\"\"\n    Retrieve experience from file\n    \"\"\"\n    with open(self.filepath) as f:\n        memory = json.load(f)\n    return memory[observation]\n</code></pre>"},{"location":"runtimes/","title":"Runtimes","text":""},{"location":"runtimes/#adala.runtimes.base.AsyncRuntime","title":"<code>AsyncRuntime</code>","text":"<p>             Bases: <code>Runtime</code></p> <p>Async version of runtime that uses asyncio to process batch of records.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>class AsyncRuntime(Runtime):\n    \"\"\"Async version of runtime that uses asyncio to process batch of records.\"\"\"\n\n    @abstractmethod\n    async def record_to_record(\n        self,\n        record: Dict[str, str],\n        input_template: str,\n        instructions_template: str,\n        output_template: str,\n        extra_fields: Optional[Dict[str, Any]] = None,\n        field_schema: Optional[Dict] = None,\n        instructions_first: bool = True,\n    ) -&gt; Dict[str, str]:\n        \"\"\"\n        Processes a record.\n\n        Args:\n            record (Dict[str, str]): The record to process.\n            input_template (str): The input template.\n            instructions_template (str): The instructions template.\n            output_template (str): The output template.\n            extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n            field_schema (Optional[Dict]): Field JSON schema to use in the templates. Defaults to all fields are strings,\n                i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n            instructions_first (bool): Whether to put instructions first. Defaults to True.\n\n        Returns:\n            Dict[str, str]: The processed record.\n        \"\"\"\n\n    @abstractmethod\n    async def batch_to_batch(\n        self,\n        batch: InternalDataFrame,\n        input_template: str,\n        instructions_template: str,\n        output_template: str,\n        extra_fields: Optional[Dict[str, str]] = None,\n        field_schema: Optional[Dict] = None,\n        instructions_first: bool = True,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Processes a record.\n\n        Args:\n            batch (InternalDataFrame): The batch to process.\n            input_template (str): The input template.\n            instructions_template (str): The instructions template.\n            output_template (str): The output template.\n            extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n            field_schema (Optional[Dict]): Field JSON schema to use in the templates. Defaults to all fields are strings,\n                i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n            instructions_first (bool): Whether to put instructions first. Defaults to True.\n\n        Returns:\n            InternalDataFrame: The processed batch.\n        \"\"\"\n        output = batch.progress_apply(\n            self.record_to_record,\n            axis=1,\n            result_type=\"expand\",\n            input_template=input_template,\n            instructions_template=instructions_template,\n            output_template=output_template,\n            extra_fields=extra_fields,\n            field_schema=field_schema,\n            instructions_first=instructions_first,\n        )\n        return output\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.AsyncRuntime.batch_to_batch","title":"<code>batch_to_batch(batch, input_template, instructions_template, output_template, extra_fields=None, field_schema=None, instructions_first=True)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Processes a record.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>InternalDataFrame</code> <p>The batch to process.</p> required <code>input_template</code> <code>str</code> <p>The input template.</p> required <code>instructions_template</code> <code>str</code> <p>The instructions template.</p> required <code>output_template</code> <code>str</code> <p>The output template.</p> required <code>extra_fields</code> <code>Optional[Dict[str, str]]</code> <p>Extra fields to use in the templates. Defaults to None.</p> <code>None</code> <code>field_schema</code> <code>Optional[Dict]</code> <p>Field JSON schema to use in the templates. Defaults to all fields are strings, i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.</p> <code>None</code> <code>instructions_first</code> <code>bool</code> <p>Whether to put instructions first. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The processed batch.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>@abstractmethod\nasync def batch_to_batch(\n    self,\n    batch: InternalDataFrame,\n    input_template: str,\n    instructions_template: str,\n    output_template: str,\n    extra_fields: Optional[Dict[str, str]] = None,\n    field_schema: Optional[Dict] = None,\n    instructions_first: bool = True,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Processes a record.\n\n    Args:\n        batch (InternalDataFrame): The batch to process.\n        input_template (str): The input template.\n        instructions_template (str): The instructions template.\n        output_template (str): The output template.\n        extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n        field_schema (Optional[Dict]): Field JSON schema to use in the templates. Defaults to all fields are strings,\n            i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n        instructions_first (bool): Whether to put instructions first. Defaults to True.\n\n    Returns:\n        InternalDataFrame: The processed batch.\n    \"\"\"\n    output = batch.progress_apply(\n        self.record_to_record,\n        axis=1,\n        result_type=\"expand\",\n        input_template=input_template,\n        instructions_template=instructions_template,\n        output_template=output_template,\n        extra_fields=extra_fields,\n        field_schema=field_schema,\n        instructions_first=instructions_first,\n    )\n    return output\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.AsyncRuntime.record_to_record","title":"<code>record_to_record(record, input_template, instructions_template, output_template, extra_fields=None, field_schema=None, instructions_first=True)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Processes a record.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Dict[str, str]</code> <p>The record to process.</p> required <code>input_template</code> <code>str</code> <p>The input template.</p> required <code>instructions_template</code> <code>str</code> <p>The instructions template.</p> required <code>output_template</code> <code>str</code> <p>The output template.</p> required <code>extra_fields</code> <code>Optional[Dict[str, str]]</code> <p>Extra fields to use in the templates. Defaults to None.</p> <code>None</code> <code>field_schema</code> <code>Optional[Dict]</code> <p>Field JSON schema to use in the templates. Defaults to all fields are strings, i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.</p> <code>None</code> <code>instructions_first</code> <code>bool</code> <p>Whether to put instructions first. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: The processed record.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>@abstractmethod\nasync def record_to_record(\n    self,\n    record: Dict[str, str],\n    input_template: str,\n    instructions_template: str,\n    output_template: str,\n    extra_fields: Optional[Dict[str, Any]] = None,\n    field_schema: Optional[Dict] = None,\n    instructions_first: bool = True,\n) -&gt; Dict[str, str]:\n    \"\"\"\n    Processes a record.\n\n    Args:\n        record (Dict[str, str]): The record to process.\n        input_template (str): The input template.\n        instructions_template (str): The instructions template.\n        output_template (str): The output template.\n        extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n        field_schema (Optional[Dict]): Field JSON schema to use in the templates. Defaults to all fields are strings,\n            i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n        instructions_first (bool): Whether to put instructions first. Defaults to True.\n\n    Returns:\n        Dict[str, str]: The processed record.\n    \"\"\"\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.Runtime","title":"<code>Runtime</code>","text":"<p>             Bases: <code>BaseModelInRegistry</code></p> <p>Base class representing a generic runtime environment.</p> <p>Attributes:</p> Name Type Description <code>verbose</code> <code>bool</code> <p>Flag indicating if runtime outputs should be verbose. Defaults to False.</p> <code>batch_size</code> <code>Optional[int]</code> <p>The batch size to use for processing records. Defaults to None.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>class Runtime(BaseModelInRegistry):\n    \"\"\"\n    Base class representing a generic runtime environment.\n\n    Attributes:\n        verbose (bool): Flag indicating if runtime outputs should be verbose. Defaults to False.\n        batch_size (Optional[int]): The batch size to use for processing records. Defaults to None.\n    \"\"\"\n\n    verbose: bool = False\n    batch_size: Optional[int] = None\n\n    @model_validator(mode=\"after\")\n    def init_runtime(self) -&gt; \"Runtime\":\n        \"\"\"Initializes the runtime.\n\n        This method should be used to validate and potentially initialize the runtime instance.\n\n        Returns:\n            Runtime: The initialized runtime instance.\n        \"\"\"\n        return self\n\n    @abstractmethod\n    def record_to_record(\n        self,\n        record: Dict[str, str],\n        input_template: str,\n        instructions_template: str,\n        output_template: str,\n        extra_fields: Optional[Dict[str, Any]] = None,\n        field_schema: Optional[Dict] = None,\n        instructions_first: bool = True,\n    ) -&gt; Dict[str, str]:\n        \"\"\"\n        Processes a record.\n\n        Args:\n            record (Dict[str, str]): The record to process.\n            input_template (str): The input template.\n            instructions_template (str): The instructions template.\n            output_template (str): The output template.\n            extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n            field_schema (Optional[Dict]): Field JSON schema to use in the templates. Defaults to all fields are strings,\n                i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n            instructions_first (bool): Whether to put instructions first. Defaults to True.\n\n        Returns:\n            Dict[str, str]: The processed record.\n        \"\"\"\n\n    def batch_to_batch(\n        self,\n        batch: InternalDataFrame,\n        input_template: str,\n        instructions_template: str,\n        output_template: str,\n        extra_fields: Optional[Dict[str, str]] = None,\n        field_schema: Optional[Dict] = None,\n        instructions_first: bool = True,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Processes a record.\n\n        Args:\n            batch (InternalDataFrame): The batch to process.\n            input_template (str): The input template.\n            instructions_template (str): The instructions template.\n            output_template (str): The output template.\n            extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n            field_schema (Optional[Dict]): Field JSON schema to use in the templates. Defaults to all fields are strings,\n                i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n            instructions_first (bool): Whether to put instructions first. Defaults to True.\n\n        Returns:\n            InternalDataFrame: The processed batch.\n        \"\"\"\n        output = batch.progress_apply(\n            self.record_to_record,\n            axis=1,\n            result_type=\"expand\",\n            input_template=input_template,\n            instructions_template=instructions_template,\n            output_template=output_template,\n            extra_fields=extra_fields,\n            field_schema=field_schema,\n            instructions_first=instructions_first,\n        )\n        return output\n\n    def record_to_batch(\n        self,\n        record: Dict[str, str],\n        input_template: str,\n        instructions_template: str,\n        output_template: str,\n        output_batch_size: int = 1,\n        extra_fields: Optional[Dict[str, str]] = None,\n        field_schema: Optional[Dict] = None,\n        instructions_first: bool = True,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Processes a record and return a batch.\n\n        Args:\n            record (Dict[str, str]): The record to process.\n            input_template (str): The input template.\n            instructions_template (str): The instructions template.\n            output_template (str): The output template.\n            output_batch_size (int): The batch size for the output. Defaults to 1.\n            extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n            field_schema (Optional[Dict]): Field JSON schema to use in the templates. Defaults to all fields are strings,\n                i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n            instructions_first (bool): Whether to put instructions first. Defaults to True.\n\n        Returns:\n            InternalDataFrame: The processed batch.\n        \"\"\"\n        batch = InternalDataFrame([record] * output_batch_size)\n        return self.batch_to_batch(\n            batch=batch,\n            input_template=input_template,\n            instructions_template=instructions_template,\n            output_template=output_template,\n            extra_fields=extra_fields,\n            field_schema=field_schema,\n            instructions_first=instructions_first,\n        )\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.Runtime.batch_to_batch","title":"<code>batch_to_batch(batch, input_template, instructions_template, output_template, extra_fields=None, field_schema=None, instructions_first=True)</code>","text":"<p>Processes a record.</p> <p>Parameters:</p> Name Type Description Default <code>batch</code> <code>InternalDataFrame</code> <p>The batch to process.</p> required <code>input_template</code> <code>str</code> <p>The input template.</p> required <code>instructions_template</code> <code>str</code> <p>The instructions template.</p> required <code>output_template</code> <code>str</code> <p>The output template.</p> required <code>extra_fields</code> <code>Optional[Dict[str, str]]</code> <p>Extra fields to use in the templates. Defaults to None.</p> <code>None</code> <code>field_schema</code> <code>Optional[Dict]</code> <p>Field JSON schema to use in the templates. Defaults to all fields are strings, i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.</p> <code>None</code> <code>instructions_first</code> <code>bool</code> <p>Whether to put instructions first. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The processed batch.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>def batch_to_batch(\n    self,\n    batch: InternalDataFrame,\n    input_template: str,\n    instructions_template: str,\n    output_template: str,\n    extra_fields: Optional[Dict[str, str]] = None,\n    field_schema: Optional[Dict] = None,\n    instructions_first: bool = True,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Processes a record.\n\n    Args:\n        batch (InternalDataFrame): The batch to process.\n        input_template (str): The input template.\n        instructions_template (str): The instructions template.\n        output_template (str): The output template.\n        extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n        field_schema (Optional[Dict]): Field JSON schema to use in the templates. Defaults to all fields are strings,\n            i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n        instructions_first (bool): Whether to put instructions first. Defaults to True.\n\n    Returns:\n        InternalDataFrame: The processed batch.\n    \"\"\"\n    output = batch.progress_apply(\n        self.record_to_record,\n        axis=1,\n        result_type=\"expand\",\n        input_template=input_template,\n        instructions_template=instructions_template,\n        output_template=output_template,\n        extra_fields=extra_fields,\n        field_schema=field_schema,\n        instructions_first=instructions_first,\n    )\n    return output\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.Runtime.init_runtime","title":"<code>init_runtime()</code>","text":"<p>Initializes the runtime.</p> <p>This method should be used to validate and potentially initialize the runtime instance.</p> <p>Returns:</p> Name Type Description <code>Runtime</code> <code>Runtime</code> <p>The initialized runtime instance.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>@model_validator(mode=\"after\")\ndef init_runtime(self) -&gt; \"Runtime\":\n    \"\"\"Initializes the runtime.\n\n    This method should be used to validate and potentially initialize the runtime instance.\n\n    Returns:\n        Runtime: The initialized runtime instance.\n    \"\"\"\n    return self\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.Runtime.record_to_batch","title":"<code>record_to_batch(record, input_template, instructions_template, output_template, output_batch_size=1, extra_fields=None, field_schema=None, instructions_first=True)</code>","text":"<p>Processes a record and return a batch.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Dict[str, str]</code> <p>The record to process.</p> required <code>input_template</code> <code>str</code> <p>The input template.</p> required <code>instructions_template</code> <code>str</code> <p>The instructions template.</p> required <code>output_template</code> <code>str</code> <p>The output template.</p> required <code>output_batch_size</code> <code>int</code> <p>The batch size for the output. Defaults to 1.</p> <code>1</code> <code>extra_fields</code> <code>Optional[Dict[str, str]]</code> <p>Extra fields to use in the templates. Defaults to None.</p> <code>None</code> <code>field_schema</code> <code>Optional[Dict]</code> <p>Field JSON schema to use in the templates. Defaults to all fields are strings, i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.</p> <code>None</code> <code>instructions_first</code> <code>bool</code> <p>Whether to put instructions first. Defaults to True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The processed batch.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>def record_to_batch(\n    self,\n    record: Dict[str, str],\n    input_template: str,\n    instructions_template: str,\n    output_template: str,\n    output_batch_size: int = 1,\n    extra_fields: Optional[Dict[str, str]] = None,\n    field_schema: Optional[Dict] = None,\n    instructions_first: bool = True,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Processes a record and return a batch.\n\n    Args:\n        record (Dict[str, str]): The record to process.\n        input_template (str): The input template.\n        instructions_template (str): The instructions template.\n        output_template (str): The output template.\n        output_batch_size (int): The batch size for the output. Defaults to 1.\n        extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n        field_schema (Optional[Dict]): Field JSON schema to use in the templates. Defaults to all fields are strings,\n            i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n        instructions_first (bool): Whether to put instructions first. Defaults to True.\n\n    Returns:\n        InternalDataFrame: The processed batch.\n    \"\"\"\n    batch = InternalDataFrame([record] * output_batch_size)\n    return self.batch_to_batch(\n        batch=batch,\n        input_template=input_template,\n        instructions_template=instructions_template,\n        output_template=output_template,\n        extra_fields=extra_fields,\n        field_schema=field_schema,\n        instructions_first=instructions_first,\n    )\n</code></pre>"},{"location":"runtimes/#adala.runtimes.base.Runtime.record_to_record","title":"<code>record_to_record(record, input_template, instructions_template, output_template, extra_fields=None, field_schema=None, instructions_first=True)</code>  <code>abstractmethod</code>","text":"<p>Processes a record.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Dict[str, str]</code> <p>The record to process.</p> required <code>input_template</code> <code>str</code> <p>The input template.</p> required <code>instructions_template</code> <code>str</code> <p>The instructions template.</p> required <code>output_template</code> <code>str</code> <p>The output template.</p> required <code>extra_fields</code> <code>Optional[Dict[str, str]]</code> <p>Extra fields to use in the templates. Defaults to None.</p> <code>None</code> <code>field_schema</code> <code>Optional[Dict]</code> <p>Field JSON schema to use in the templates. Defaults to all fields are strings, i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.</p> <code>None</code> <code>instructions_first</code> <code>bool</code> <p>Whether to put instructions first. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: The processed record.</p> Source code in <code>adala/runtimes/base.py</code> <pre><code>@abstractmethod\ndef record_to_record(\n    self,\n    record: Dict[str, str],\n    input_template: str,\n    instructions_template: str,\n    output_template: str,\n    extra_fields: Optional[Dict[str, Any]] = None,\n    field_schema: Optional[Dict] = None,\n    instructions_first: bool = True,\n) -&gt; Dict[str, str]:\n    \"\"\"\n    Processes a record.\n\n    Args:\n        record (Dict[str, str]): The record to process.\n        input_template (str): The input template.\n        instructions_template (str): The instructions template.\n        output_template (str): The output template.\n        extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n        field_schema (Optional[Dict]): Field JSON schema to use in the templates. Defaults to all fields are strings,\n            i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n        instructions_first (bool): Whether to put instructions first. Defaults to True.\n\n    Returns:\n        Dict[str, str]: The processed record.\n    \"\"\"\n</code></pre>"},{"location":"runtimes/#adala.runtimes._openai.AsyncOpenAIChatRuntime","title":"<code>AsyncOpenAIChatRuntime</code>","text":"<p>             Bases: <code>AsyncRuntime</code></p> <p>Runtime that uses OpenAI API and chat completion models to perform the skill. It uses async calls to OpenAI API.</p> <p>Attributes:</p> Name Type Description <code>openai_model</code> <code>str</code> <p>OpenAI model name.</p> <code>openai_api_key</code> <code>Optional[str]</code> <p>OpenAI API key. If not provided, will be taken from OPENAI_API_KEY environment variable.</p> <code>max_tokens</code> <code>Optional[int]</code> <p>Maximum number of tokens to generate. Defaults to 1000.</p> <code>temperature</code> <code>Optional[float]</code> <p>Temperature for sampling, between 0 and 1. Higher values means the model will take more risks. Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer. Defaults to 0.0.</p> <code>concurrent_clients</code> <code>Optional[int]</code> <p>Number of concurrent clients to OpenAI API. More clients means more parallel requests, but also more money spent and more chances to hit the rate limit. Defaults to 10.</p> Source code in <code>adala/runtimes/_openai.py</code> <pre><code>class AsyncOpenAIChatRuntime(AsyncRuntime):\n    \"\"\"\n    Runtime that uses [OpenAI API](https://openai.com/) and chat completion models to perform the skill.\n    It uses async calls to OpenAI API.\n\n    Attributes:\n        openai_model: OpenAI model name.\n        openai_api_key: OpenAI API key. If not provided, will be taken from OPENAI_API_KEY environment variable.\n        max_tokens: Maximum number of tokens to generate. Defaults to 1000.\n        temperature: Temperature for sampling, between 0 and 1. Higher values means the model will take more risks.\n            Try 0.9 for more creative applications, and 0 (argmax sampling) for ones with a well-defined answer.\n            Defaults to 0.0.\n\n        concurrent_clients: Number of concurrent clients to OpenAI API. More clients means more parallel requests, but\n            also more money spent and more chances to hit the rate limit. Defaults to 10.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)  # for @computed_field\n\n    openai_model: str = Field(alias=\"model\")\n    openai_api_key: Optional[str] = Field(\n        default=os.getenv(\"OPENAI_API_KEY\"), alias=\"api_key\"\n    )\n    max_tokens: Optional[int] = 1000\n    temperature: Optional[float] = 0.0\n    splitter: Optional[str] = None\n    concurrent_clients: Optional[int] = 10\n    timeout: Optional[int] = 10\n\n    @computed_field\n    def _client(self) -&gt; AsyncOpenAI:\n        return AsyncOpenAI(\n            api_key=self.openai_api_key,\n            http_client=httpx.AsyncClient(\n                limits=httpx.Limits(\n                    max_connections=self.concurrent_clients,\n                    max_keepalive_connections=self.concurrent_clients,\n                ),\n                timeout=self.timeout,\n            ),\n        )\n\n    def init_runtime(self) -&gt; \"Runtime\":\n        # check model availability\n        try:\n            _client = OpenAI(api_key=self.openai_api_key)\n            _client.models.retrieve(self.openai_model)\n        except NotFoundError:\n            raise ValueError(\n                f'Requested model \"{self.openai_model}\" is not available in your OpenAI account.'\n            )\n        return self\n\n    def _prepare_prompt(\n        self,\n        row,\n        input_template: str,\n        instructions_template: str,\n        suffix: str,\n        extra_fields: dict,\n    ) -&gt; Dict[str, str]:\n        \"\"\"Prepare input prompt for OpenAI API from the row of the dataframe\"\"\"\n        return {\n            \"system\": instructions_template,\n            \"user\": input_template.format(**row, **extra_fields) + suffix,\n        }\n\n    async def batch_to_batch(\n        self,\n        batch: InternalDataFrame,\n        input_template: str,\n        instructions_template: str,\n        output_template: str,\n        extra_fields: Optional[Dict[str, str]] = None,\n        field_schema: Optional[Dict] = None,\n        instructions_first: bool = True,\n    ) -&gt; InternalDataFrame:\n        \"\"\"Execute batch of requests with async calls to OpenAI API\"\"\"\n\n        extra_fields = extra_fields or {}\n        field_schema = field_schema or {}\n\n        options = {}\n        for field, schema in field_schema.items():\n            if schema.get(\"type\") == \"array\":\n                options[field] = schema.get(\"items\", {}).get(\"enum\", [])\n\n        output_fields = parse_template(\n            partial_str_format(output_template, **extra_fields), include_texts=True\n        )\n\n        if len(output_fields) &gt; 2:\n            raise NotImplementedError(\"Only one output field is supported\")\n\n        suffix = \"\"\n        outputs = []\n        for output_field in output_fields:\n            if output_field[\"type\"] == \"text\":\n                suffix += output_field[\"text\"]\n\n            elif output_field[\"type\"] == \"var\":\n                name = output_field[\"text\"]\n                # prepare prompts\n                prompts = batch.apply(\n                    lambda row: self._prepare_prompt(\n                        row, input_template, instructions_template, suffix, extra_fields\n                    ),\n                    axis=1,\n                ).tolist()\n\n                responses = await async_concurrent_create_completion(\n                    prompts=prompts,\n                    client=self._client,\n                    instruction_first=instructions_first,\n                    max_tokens=self.max_tokens,\n                    temperature=self.temperature,\n                    openai_model=self.openai_model,\n                )\n\n                # parse responses, optionally match it with options\n                for prompt, response in zip(prompts, responses):\n                    completion_text = response.pop(\"text\")\n                    if self.verbose:\n                        if response[\"error\"] is not None:\n                            print_error(\n                                f\"Prompt: {prompt}\\nOpenAI API error: {response}\"\n                            )\n                        else:\n                            print(\n                                f\"Prompt: {prompt}\\nOpenAI API response: {completion_text}\"\n                            )\n                    if name in options and completion_text is not None:\n                        completion_text = match_options(completion_text, options[name])\n                        # still technically possible to have a name collision here with the error, message, details fields\n                        response[name] = completion_text\n                    outputs.append(response)\n\n        # TODO: note that this doesn't work for multiple output fields e.g. `Output {output1} and Output {output2}`\n        output_df = InternalDataFrame(outputs)\n        return output_df.set_index(batch.index)\n\n    async def record_to_record(\n        self,\n        record: Dict[str, str],\n        input_template: str,\n        instructions_template: str,\n        output_template: str,\n        extra_fields: Optional[Dict[str, Any]] = None,\n        field_schema: Optional[Dict] = None,\n        instructions_first: bool = True,\n    ) -&gt; Dict[str, str]:\n        raise NotImplementedError(\"record_to_record is not implemented\")\n</code></pre>"},{"location":"runtimes/#adala.runtimes._openai.AsyncOpenAIChatRuntime.batch_to_batch","title":"<code>batch_to_batch(batch, input_template, instructions_template, output_template, extra_fields=None, field_schema=None, instructions_first=True)</code>  <code>async</code>","text":"<p>Execute batch of requests with async calls to OpenAI API</p> Source code in <code>adala/runtimes/_openai.py</code> <pre><code>async def batch_to_batch(\n    self,\n    batch: InternalDataFrame,\n    input_template: str,\n    instructions_template: str,\n    output_template: str,\n    extra_fields: Optional[Dict[str, str]] = None,\n    field_schema: Optional[Dict] = None,\n    instructions_first: bool = True,\n) -&gt; InternalDataFrame:\n    \"\"\"Execute batch of requests with async calls to OpenAI API\"\"\"\n\n    extra_fields = extra_fields or {}\n    field_schema = field_schema or {}\n\n    options = {}\n    for field, schema in field_schema.items():\n        if schema.get(\"type\") == \"array\":\n            options[field] = schema.get(\"items\", {}).get(\"enum\", [])\n\n    output_fields = parse_template(\n        partial_str_format(output_template, **extra_fields), include_texts=True\n    )\n\n    if len(output_fields) &gt; 2:\n        raise NotImplementedError(\"Only one output field is supported\")\n\n    suffix = \"\"\n    outputs = []\n    for output_field in output_fields:\n        if output_field[\"type\"] == \"text\":\n            suffix += output_field[\"text\"]\n\n        elif output_field[\"type\"] == \"var\":\n            name = output_field[\"text\"]\n            # prepare prompts\n            prompts = batch.apply(\n                lambda row: self._prepare_prompt(\n                    row, input_template, instructions_template, suffix, extra_fields\n                ),\n                axis=1,\n            ).tolist()\n\n            responses = await async_concurrent_create_completion(\n                prompts=prompts,\n                client=self._client,\n                instruction_first=instructions_first,\n                max_tokens=self.max_tokens,\n                temperature=self.temperature,\n                openai_model=self.openai_model,\n            )\n\n            # parse responses, optionally match it with options\n            for prompt, response in zip(prompts, responses):\n                completion_text = response.pop(\"text\")\n                if self.verbose:\n                    if response[\"error\"] is not None:\n                        print_error(\n                            f\"Prompt: {prompt}\\nOpenAI API error: {response}\"\n                        )\n                    else:\n                        print(\n                            f\"Prompt: {prompt}\\nOpenAI API response: {completion_text}\"\n                        )\n                if name in options and completion_text is not None:\n                    completion_text = match_options(completion_text, options[name])\n                    # still technically possible to have a name collision here with the error, message, details fields\n                    response[name] = completion_text\n                outputs.append(response)\n\n    # TODO: note that this doesn't work for multiple output fields e.g. `Output {output1} and Output {output2}`\n    output_df = InternalDataFrame(outputs)\n    return output_df.set_index(batch.index)\n</code></pre>"},{"location":"runtimes/#adala.runtimes._openai.OpenAIChatRuntime","title":"<code>OpenAIChatRuntime</code>","text":"<p>             Bases: <code>Runtime</code></p> <p>Runtime that uses OpenAI API and chat completion models to perform the skill.</p> <p>Attributes:</p> Name Type Description <code>openai_model</code> <code>str</code> <p>OpenAI model name.</p> <code>openai_api_key</code> <code>Optional[str]</code> <p>OpenAI API key. If not provided, will be taken from OPENAI_API_KEY environment variable.</p> <code>max_tokens</code> <code>Optional[int]</code> <p>Maximum number of tokens to generate. Defaults to 1000.</p> Source code in <code>adala/runtimes/_openai.py</code> <pre><code>class OpenAIChatRuntime(Runtime):\n    \"\"\"\n    Runtime that uses [OpenAI API](https://openai.com/) and chat completion models to perform the skill.\n\n    Attributes:\n        openai_model: OpenAI model name.\n        openai_api_key: OpenAI API key. If not provided, will be taken from OPENAI_API_KEY environment variable.\n        max_tokens: Maximum number of tokens to generate. Defaults to 1000.\n    \"\"\"\n\n    model_config = ConfigDict(arbitrary_types_allowed=True)  # for @computed_field\n\n    openai_model: str = Field(alias=\"model\")\n    openai_api_key: Optional[str] = Field(\n        default=os.getenv(\"OPENAI_API_KEY\"), alias=\"api_key\"\n    )\n    max_tokens: Optional[int] = 1000\n    splitter: Optional[str] = None\n\n    @computed_field\n    def _client(self) -&gt; OpenAI:\n        return OpenAI(api_key=self.openai_api_key)\n\n    def init_runtime(self) -&gt; \"Runtime\":\n        # check model availability\n        try:\n            self._client.models.retrieve(self.openai_model)\n        except NotFoundError:\n            raise ValueError(\n                f'Requested model \"{self.openai_model}\" is not available in your OpenAI account.'\n            )\n        return self\n\n    def execute(self, messages: List):\n        \"\"\"\n        Execute OpenAI request given list of messages in OpenAI API format\n        \"\"\"\n        if self.verbose:\n            print(f\"OpenAI request: {messages}\")\n\n        completion = self._client.chat.completions.create(\n            model=self.openai_model, messages=messages\n        )\n        completion_text = completion.choices[0].message.content\n\n        if self.verbose:\n            print(f\"OpenAI response: {completion_text}\")\n        return completion_text\n\n    def record_to_record(\n        self,\n        record: Dict[str, str],\n        input_template: str,\n        instructions_template: str,\n        output_template: str,\n        extra_fields: Optional[Dict[str, str]] = None,\n        field_schema: Optional[Dict] = None,\n        instructions_first: bool = False,\n    ) -&gt; Dict[str, str]:\n        \"\"\"\n        Execute OpenAI request given record and templates for input, instructions and output.\n\n        Args:\n            record: Record to be used for input, instructions and output templates.\n            input_template: Template for input message.\n            instructions_template: Template for instructions message.\n            output_template: Template for output message.\n            extra_fields: Extra fields to be used in templates.\n            field_schema: Field schema to be used for parsing templates.\n            instructions_first: If True, instructions will be sent before input.\n\n        Returns:\n            Dict[str, str]: Output record.\n        \"\"\"\n\n        extra_fields = extra_fields or {}\n        field_schema = field_schema or {}\n\n        options = {}\n        for field, schema in field_schema.items():\n            if schema.get(\"type\") == \"array\":\n                options[field] = schema.get(\"items\", {}).get(\"enum\", [])\n\n        output_fields = parse_template(\n            partial_str_format(output_template, **extra_fields), include_texts=True\n        )\n        system_prompt = instructions_template\n        user_prompt = input_template.format(**record, **extra_fields)\n        messages = [{\"role\": \"system\", \"content\": system_prompt}]\n\n        outputs = {}\n        for output_field in output_fields:\n            if output_field[\"type\"] == \"text\":\n                if user_prompt is not None:\n                    user_prompt += f\"\\n{output_field['text']}\"\n                else:\n                    user_prompt = output_field[\"text\"]\n            elif output_field[\"type\"] == \"var\":\n                name = output_field[\"text\"]\n                messages.append({\"role\": \"user\", \"content\": user_prompt})\n                completion_text = self.execute(messages)\n                if name in options:\n                    completion_text = match_options(completion_text, options[name])\n                outputs[name] = completion_text\n                messages.append({\"role\": \"assistant\", \"content\": completion_text})\n                user_prompt = None\n\n        return outputs\n</code></pre>"},{"location":"runtimes/#adala.runtimes._openai.OpenAIChatRuntime.execute","title":"<code>execute(messages)</code>","text":"<p>Execute OpenAI request given list of messages in OpenAI API format</p> Source code in <code>adala/runtimes/_openai.py</code> <pre><code>def execute(self, messages: List):\n    \"\"\"\n    Execute OpenAI request given list of messages in OpenAI API format\n    \"\"\"\n    if self.verbose:\n        print(f\"OpenAI request: {messages}\")\n\n    completion = self._client.chat.completions.create(\n        model=self.openai_model, messages=messages\n    )\n    completion_text = completion.choices[0].message.content\n\n    if self.verbose:\n        print(f\"OpenAI response: {completion_text}\")\n    return completion_text\n</code></pre>"},{"location":"runtimes/#adala.runtimes._openai.OpenAIChatRuntime.record_to_record","title":"<code>record_to_record(record, input_template, instructions_template, output_template, extra_fields=None, field_schema=None, instructions_first=False)</code>","text":"<p>Execute OpenAI request given record and templates for input, instructions and output.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Dict[str, str]</code> <p>Record to be used for input, instructions and output templates.</p> required <code>input_template</code> <code>str</code> <p>Template for input message.</p> required <code>instructions_template</code> <code>str</code> <p>Template for instructions message.</p> required <code>output_template</code> <code>str</code> <p>Template for output message.</p> required <code>extra_fields</code> <code>Optional[Dict[str, str]]</code> <p>Extra fields to be used in templates.</p> <code>None</code> <code>field_schema</code> <code>Optional[Dict]</code> <p>Field schema to be used for parsing templates.</p> <code>None</code> <code>instructions_first</code> <code>bool</code> <p>If True, instructions will be sent before input.</p> <code>False</code> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: Output record.</p> Source code in <code>adala/runtimes/_openai.py</code> <pre><code>def record_to_record(\n    self,\n    record: Dict[str, str],\n    input_template: str,\n    instructions_template: str,\n    output_template: str,\n    extra_fields: Optional[Dict[str, str]] = None,\n    field_schema: Optional[Dict] = None,\n    instructions_first: bool = False,\n) -&gt; Dict[str, str]:\n    \"\"\"\n    Execute OpenAI request given record and templates for input, instructions and output.\n\n    Args:\n        record: Record to be used for input, instructions and output templates.\n        input_template: Template for input message.\n        instructions_template: Template for instructions message.\n        output_template: Template for output message.\n        extra_fields: Extra fields to be used in templates.\n        field_schema: Field schema to be used for parsing templates.\n        instructions_first: If True, instructions will be sent before input.\n\n    Returns:\n        Dict[str, str]: Output record.\n    \"\"\"\n\n    extra_fields = extra_fields or {}\n    field_schema = field_schema or {}\n\n    options = {}\n    for field, schema in field_schema.items():\n        if schema.get(\"type\") == \"array\":\n            options[field] = schema.get(\"items\", {}).get(\"enum\", [])\n\n    output_fields = parse_template(\n        partial_str_format(output_template, **extra_fields), include_texts=True\n    )\n    system_prompt = instructions_template\n    user_prompt = input_template.format(**record, **extra_fields)\n    messages = [{\"role\": \"system\", \"content\": system_prompt}]\n\n    outputs = {}\n    for output_field in output_fields:\n        if output_field[\"type\"] == \"text\":\n            if user_prompt is not None:\n                user_prompt += f\"\\n{output_field['text']}\"\n            else:\n                user_prompt = output_field[\"text\"]\n        elif output_field[\"type\"] == \"var\":\n            name = output_field[\"text\"]\n            messages.append({\"role\": \"user\", \"content\": user_prompt})\n            completion_text = self.execute(messages)\n            if name in options:\n                completion_text = match_options(completion_text, options[name])\n            outputs[name] = completion_text\n            messages.append({\"role\": \"assistant\", \"content\": completion_text})\n            user_prompt = None\n\n    return outputs\n</code></pre>"},{"location":"runtimes/#adala.runtimes._openai.OpenAIVisionRuntime","title":"<code>OpenAIVisionRuntime</code>","text":"<p>             Bases: <code>OpenAIChatRuntime</code></p> <p>Runtime that uses OpenAI API and vision models to perform the skill. Only compatible with OpenAI API version 1.0.0 or higher.</p> Source code in <code>adala/runtimes/_openai.py</code> <pre><code>class OpenAIVisionRuntime(OpenAIChatRuntime):\n    \"\"\"\n    Runtime that uses [OpenAI API](https://openai.com/) and vision models to perform the skill.\n    Only compatible with OpenAI API version 1.0.0 or higher.\n    \"\"\"\n\n    def record_to_record(\n        self,\n        record: Dict[str, str],\n        input_template: str,\n        instructions_template: str,\n        output_template: str,\n        extra_fields: Optional[Dict[str, str]] = None,\n        field_schema: Optional[Dict] = None,\n        instructions_first: bool = False,\n    ) -&gt; Dict[str, str]:\n        \"\"\"\n        Execute OpenAI request given record and templates for input, instructions and output.\n\n        Args:\n            record: Record to be used for input, instructions and output templates.\n            input_template: Template for input message.\n            instructions_template: Template for instructions message.\n            output_template: Template for output message.\n            extra_fields: Extra fields to be used in templates.\n            field_schema: Field jsonschema to be used for parsing templates.\n                         Field schema must contain \"format\": \"uri\" for image fields. For example:\n                            ```json\n                            {\n                                \"image\": {\n                                    \"type\": \"string\",\n                                    \"format\": \"uri\"\n                                }\n                            }\n                            ```\n            instructions_first: If True, instructions will be sent before input.\n        \"\"\"\n\n        if not check_if_new_openai_version():\n            raise NotImplementedError(\n                f\"{self.__class__.__name__} requires OpenAI API version 1.0.0 or higher.\"\n            )\n\n        extra_fields = extra_fields or {}\n        field_schema = field_schema or {}\n\n        output_fields = parse_template(\n            partial_str_format(output_template, **extra_fields), include_texts=False\n        )\n\n        if len(output_fields) &gt; 1:\n            raise NotImplementedError(\n                f\"{self.__class__.__name__} does not support multiple output fields. \"\n                f\"Found: {output_fields}\"\n            )\n        output_field = output_fields[0]\n        output_field_name = output_field[\"text\"]\n\n        input_fields = parse_template(input_template)\n\n        # split input template into text and image parts\n        input_text = \"\"\n        content = [\n            {\n                \"type\": \"text\",\n                \"text\": instructions_template,\n            }\n        ]\n        for field in input_fields:\n            if field[\"type\"] == \"text\":\n                input_text += field[\"text\"]\n            elif field[\"type\"] == \"var\":\n                if field[\"text\"] not in field_schema:\n                    input_text += record[field[\"text\"]]\n                elif field_schema[field[\"text\"]][\"type\"] == \"string\":\n                    if field_schema[field[\"text\"]].get(\"format\") == \"uri\":\n                        if input_text:\n                            content.append({\"type\": \"text\", \"text\": input_text})\n                            input_text = \"\"\n                        content.append(\n                            {\"type\": \"image_url\", \"image_url\": record[field[\"text\"]]}\n                        )\n                    else:\n                        input_text += record[field[\"text\"]]\n                else:\n                    raise ValueError(\n                        f'Unsupported field type: {field_schema[field[\"text\"]][\"type\"]}'\n                    )\n        if input_text:\n            content.append({\"type\": \"text\", \"text\": input_text})\n\n        if self.verbose:\n            print(f\"**Prompt content**:\\n{content}\")\n\n        completion = self._client.chat.completions.create(\n            model=self.openai_model,\n            messages=[{\"role\": \"user\", \"content\": content}],\n            max_tokens=self.max_tokens,\n        )\n\n        completion_text = completion.choices[0].message.content\n        return {output_field_name: completion_text}\n</code></pre>"},{"location":"runtimes/#adala.runtimes._openai.OpenAIVisionRuntime.record_to_record","title":"<code>record_to_record(record, input_template, instructions_template, output_template, extra_fields=None, field_schema=None, instructions_first=False)</code>","text":"<p>Execute OpenAI request given record and templates for input, instructions and output.</p> <p>Parameters:</p> Name Type Description Default <code>record</code> <code>Dict[str, str]</code> <p>Record to be used for input, instructions and output templates.</p> required <code>input_template</code> <code>str</code> <p>Template for input message.</p> required <code>instructions_template</code> <code>str</code> <p>Template for instructions message.</p> required <code>output_template</code> <code>str</code> <p>Template for output message.</p> required <code>extra_fields</code> <code>Optional[Dict[str, str]]</code> <p>Extra fields to be used in templates.</p> <code>None</code> <code>field_schema</code> <code>Optional[Dict]</code> <p>Field jsonschema to be used for parsing templates.          Field schema must contain \"format\": \"uri\" for image fields. For example:             <code>json             {                 \"image\": {                     \"type\": \"string\",                     \"format\": \"uri\"                 }             }</code></p> <code>None</code> <code>instructions_first</code> <code>bool</code> <p>If True, instructions will be sent before input.</p> <code>False</code> Source code in <code>adala/runtimes/_openai.py</code> <pre><code>def record_to_record(\n    self,\n    record: Dict[str, str],\n    input_template: str,\n    instructions_template: str,\n    output_template: str,\n    extra_fields: Optional[Dict[str, str]] = None,\n    field_schema: Optional[Dict] = None,\n    instructions_first: bool = False,\n) -&gt; Dict[str, str]:\n    \"\"\"\n    Execute OpenAI request given record and templates for input, instructions and output.\n\n    Args:\n        record: Record to be used for input, instructions and output templates.\n        input_template: Template for input message.\n        instructions_template: Template for instructions message.\n        output_template: Template for output message.\n        extra_fields: Extra fields to be used in templates.\n        field_schema: Field jsonschema to be used for parsing templates.\n                     Field schema must contain \"format\": \"uri\" for image fields. For example:\n                        ```json\n                        {\n                            \"image\": {\n                                \"type\": \"string\",\n                                \"format\": \"uri\"\n                            }\n                        }\n                        ```\n        instructions_first: If True, instructions will be sent before input.\n    \"\"\"\n\n    if not check_if_new_openai_version():\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} requires OpenAI API version 1.0.0 or higher.\"\n        )\n\n    extra_fields = extra_fields or {}\n    field_schema = field_schema or {}\n\n    output_fields = parse_template(\n        partial_str_format(output_template, **extra_fields), include_texts=False\n    )\n\n    if len(output_fields) &gt; 1:\n        raise NotImplementedError(\n            f\"{self.__class__.__name__} does not support multiple output fields. \"\n            f\"Found: {output_fields}\"\n        )\n    output_field = output_fields[0]\n    output_field_name = output_field[\"text\"]\n\n    input_fields = parse_template(input_template)\n\n    # split input template into text and image parts\n    input_text = \"\"\n    content = [\n        {\n            \"type\": \"text\",\n            \"text\": instructions_template,\n        }\n    ]\n    for field in input_fields:\n        if field[\"type\"] == \"text\":\n            input_text += field[\"text\"]\n        elif field[\"type\"] == \"var\":\n            if field[\"text\"] not in field_schema:\n                input_text += record[field[\"text\"]]\n            elif field_schema[field[\"text\"]][\"type\"] == \"string\":\n                if field_schema[field[\"text\"]].get(\"format\") == \"uri\":\n                    if input_text:\n                        content.append({\"type\": \"text\", \"text\": input_text})\n                        input_text = \"\"\n                    content.append(\n                        {\"type\": \"image_url\", \"image_url\": record[field[\"text\"]]}\n                    )\n                else:\n                    input_text += record[field[\"text\"]]\n            else:\n                raise ValueError(\n                    f'Unsupported field type: {field_schema[field[\"text\"]][\"type\"]}'\n                )\n    if input_text:\n        content.append({\"type\": \"text\", \"text\": input_text})\n\n    if self.verbose:\n        print(f\"**Prompt content**:\\n{content}\")\n\n    completion = self._client.chat.completions.create(\n        model=self.openai_model,\n        messages=[{\"role\": \"user\", \"content\": content}],\n        max_tokens=self.max_tokens,\n    )\n\n    completion_text = completion.choices[0].message.content\n    return {output_field_name: completion_text}\n</code></pre>"},{"location":"runtimes/#adala.runtimes._openai.async_create_completion","title":"<code>async_create_completion(model, user_prompt, client, system_prompt=None, openai_api_key=None, instruction_first=True, max_tokens=1000, temperature=0.0)</code>  <code>async</code>","text":"<p>Async version of create_completion function with error handling and session timeout.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>str</code> <p>OpenAI model name.</p> required <code>user_prompt</code> <code>str</code> <p>User prompt.</p> required <code>client</code> <code>AsyncOpenAI</code> <p>Async OpenAI client.</p> required <code>system_prompt</code> <code>str</code> <p>System prompt.</p> <code>None</code> <code>openai_api_key</code> <code>str</code> <p>OpenAI API key (if not set, will use OPENAI_API_KEY environment variable).</p> <code>None</code> <code>instruction_first</code> <code>bool</code> <p>Whether to put instructions first.</p> <code>True</code> <code>max_tokens</code> <code>int</code> <p>Maximum tokens to generate.</p> <code>1000</code> <code>temperature</code> <code>float</code> <p>Temperature for sampling.</p> <code>0.0</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: OpenAI response or error message.</p> Source code in <code>adala/runtimes/_openai.py</code> <pre><code>@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(6))\nasync def async_create_completion(\n    model: str,\n    user_prompt: str,\n    client: AsyncOpenAI,\n    system_prompt: str = None,\n    openai_api_key: str = None,\n    instruction_first: bool = True,\n    max_tokens: int = 1000,\n    temperature: float = 0.0,\n) -&gt; Dict[str, Any]:\n    \"\"\"\n    Async version of create_completion function with error handling and session timeout.\n\n    Args:\n        model: OpenAI model name.\n        user_prompt: User prompt.\n        client: Async OpenAI client.\n        system_prompt: System prompt.\n        openai_api_key: OpenAI API key (if not set, will use OPENAI_API_KEY environment variable).\n        instruction_first: Whether to put instructions first.\n        max_tokens: Maximum tokens to generate.\n        temperature: Temperature for sampling.\n\n    Returns:\n        Dict[str, Any]: OpenAI response or error message.\n    \"\"\"\n    openai_api_key = openai_api_key or os.getenv(\"OPENAI_API_KEY\")\n    messages = [{\"role\": \"user\", \"content\": user_prompt}]\n    if system_prompt:\n        if instruction_first:\n            messages.insert(0, {\"role\": \"system\", \"content\": system_prompt})\n        else:\n            messages[0][\"content\"] += system_prompt\n\n    try:\n        completion = await client.chat.completions.create(\n            model=model,\n            messages=messages,\n            max_tokens=max_tokens,\n            temperature=temperature,\n        )\n        completion_text = completion.choices[0].message.content\n        return {\n            \"text\": completion_text,\n            \"_adala_error\": False,\n            \"_adala_message\": None,\n            \"_adala_details\": None,\n        }\n    except Exception as e:\n        # Handle other exceptions\n        return {\n            \"text\": None,\n            \"_adala_error\": True,\n            \"_adala_message\": type(e).__name__,\n            \"_adala_details\": str(e),\n        }\n</code></pre>"},{"location":"skills/","title":"Skills","text":""},{"location":"skills/#adala.skills._base.AnalysisSkill","title":"<code>AnalysisSkill</code>","text":"<p>             Bases: <code>Skill</code></p> <p>Analysis skill that analyzes a dataframe and returns a record (e.g. for data analysis purposes). See base class Skill for more information about the attributes.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>class AnalysisSkill(Skill):\n    \"\"\"\n    Analysis skill that analyzes a dataframe and returns a record (e.g. for data analysis purposes).\n    See base class Skill for more information about the attributes.\n    \"\"\"\n\n    input_separator: str = \"\\n\"\n    chunk_size: Optional[int] = None\n\n    def apply(\n        self,\n        input: Union[InternalDataFrame, InternalSeries, Dict],\n        runtime: Runtime,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Applies the skill to a dataframe and returns a record.\n\n        Args:\n            input (InternalDataFrame): The input data to be processed.\n            runtime (Runtime): The runtime instance to be used for processing.\n\n        Returns:\n            InternalSeries: The record containing the analysis results.\n        \"\"\"\n        if isinstance(input, InternalSeries):\n            input = input.to_frame()\n        elif isinstance(input, dict):\n            input = InternalDataFrame([input])\n\n        extra_fields = self._get_extra_fields()\n\n        # if chunk_size is specified, split the input into chunks and process each chunk separately\n        if self.chunk_size is not None:\n            chunks = (\n                input.iloc[i : i + self.chunk_size]\n                for i in range(0, len(input), self.chunk_size)\n            )\n        else:\n            chunks = [input]\n        outputs = []\n        total = input.shape[0] // self.chunk_size if self.chunk_size is not None else 1\n        for chunk in tqdm(chunks, desc=\"Processing chunks\", total=total):\n            agg_chunk = (\n                chunk.reset_index()\n                .apply(\n                    lambda row: self.input_template.format(\n                        **row, **extra_fields, i=int(row.name) + 1\n                    ),\n                    axis=1,\n                )\n                .str.cat(sep=self.input_separator)\n            )\n            output = runtime.record_to_record(\n                {\"input\": agg_chunk},\n                input_template=\"{input}\",\n                output_template=self.output_template,\n                instructions_template=self.instructions,\n                extra_fields=extra_fields,\n                instructions_first=self.instructions_first,\n            )\n            outputs.append(InternalSeries(output))\n        output = InternalDataFrame(outputs)\n\n        return output\n\n    def improve(self, **kwargs):\n        \"\"\"\n        Improves the skill.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"skills/#adala.skills._base.AnalysisSkill.apply","title":"<code>apply(input, runtime)</code>","text":"<p>Applies the skill to a dataframe and returns a record.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>InternalDataFrame</code> <p>The input data to be processed.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime instance to be used for processing.</p> required <p>Returns:</p> Name Type Description <code>InternalSeries</code> <code>InternalDataFrame</code> <p>The record containing the analysis results.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>def apply(\n    self,\n    input: Union[InternalDataFrame, InternalSeries, Dict],\n    runtime: Runtime,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Applies the skill to a dataframe and returns a record.\n\n    Args:\n        input (InternalDataFrame): The input data to be processed.\n        runtime (Runtime): The runtime instance to be used for processing.\n\n    Returns:\n        InternalSeries: The record containing the analysis results.\n    \"\"\"\n    if isinstance(input, InternalSeries):\n        input = input.to_frame()\n    elif isinstance(input, dict):\n        input = InternalDataFrame([input])\n\n    extra_fields = self._get_extra_fields()\n\n    # if chunk_size is specified, split the input into chunks and process each chunk separately\n    if self.chunk_size is not None:\n        chunks = (\n            input.iloc[i : i + self.chunk_size]\n            for i in range(0, len(input), self.chunk_size)\n        )\n    else:\n        chunks = [input]\n    outputs = []\n    total = input.shape[0] // self.chunk_size if self.chunk_size is not None else 1\n    for chunk in tqdm(chunks, desc=\"Processing chunks\", total=total):\n        agg_chunk = (\n            chunk.reset_index()\n            .apply(\n                lambda row: self.input_template.format(\n                    **row, **extra_fields, i=int(row.name) + 1\n                ),\n                axis=1,\n            )\n            .str.cat(sep=self.input_separator)\n        )\n        output = runtime.record_to_record(\n            {\"input\": agg_chunk},\n            input_template=\"{input}\",\n            output_template=self.output_template,\n            instructions_template=self.instructions,\n            extra_fields=extra_fields,\n            instructions_first=self.instructions_first,\n        )\n        outputs.append(InternalSeries(output))\n    output = InternalDataFrame(outputs)\n\n    return output\n</code></pre>"},{"location":"skills/#adala.skills._base.AnalysisSkill.improve","title":"<code>improve(**kwargs)</code>","text":"<p>Improves the skill.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>def improve(self, **kwargs):\n    \"\"\"\n    Improves the skill.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"skills/#adala.skills._base.SampleTransformSkill","title":"<code>SampleTransformSkill</code>","text":"<p>             Bases: <code>TransformSkill</code></p> Source code in <code>adala/skills/_base.py</code> <pre><code>class SampleTransformSkill(TransformSkill):\n    sample_size: int\n\n    def apply(\n        self,\n        input: InternalDataFrame,\n        runtime: Runtime,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Applies the skill to a dataframe and returns a dataframe.\n\n        Args:\n            input (InternalDataFrame): The input data to be processed.\n            runtime (Runtime): The runtime instance to be used for processing.\n\n        Returns:\n            InternalDataFrame: The processed data.\n        \"\"\"\n        return super(SampleTransformSkill, self).apply(\n            input.sample(self.sample_size), runtime\n        )\n</code></pre>"},{"location":"skills/#adala.skills._base.SampleTransformSkill.apply","title":"<code>apply(input, runtime)</code>","text":"<p>Applies the skill to a dataframe and returns a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>InternalDataFrame</code> <p>The input data to be processed.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime instance to be used for processing.</p> required <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The processed data.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>def apply(\n    self,\n    input: InternalDataFrame,\n    runtime: Runtime,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Applies the skill to a dataframe and returns a dataframe.\n\n    Args:\n        input (InternalDataFrame): The input data to be processed.\n        runtime (Runtime): The runtime instance to be used for processing.\n\n    Returns:\n        InternalDataFrame: The processed data.\n    \"\"\"\n    return super(SampleTransformSkill, self).apply(\n        input.sample(self.sample_size), runtime\n    )\n</code></pre>"},{"location":"skills/#adala.skills._base.Skill","title":"<code>Skill</code>","text":"<p>             Bases: <code>BaseModelInRegistry</code></p> <p>Abstract base class representing a skill.</p> <p>Provides methods to interact with and obtain information about skills.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique name of the skill.</p> <code>instructions</code> <code>str</code> <p>Instructs agent what to do with the input data.</p> <code>input_template</code> <code>str</code> <p>Template for the input data.</p> <code>output_template</code> <code>str</code> <p>Template for the output data.</p> <code>description</code> <code>Optional[str]</code> <p>Description of the skill.</p> <code>field_schema</code> <code>Optional[Dict]</code> <p>Field JSON schema to use in the templates. Defaults to all fields are strings, i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.</p> <code>extra_fields</code> <code>Optional[Dict[str, str]]</code> <p>Extra fields to use in the templates. Defaults to None.</p> <code>instructions_first</code> <code>bool</code> <p>Flag indicating if instructions should be executed before input. Defaults to True.</p> <code>verbose</code> <code>bool</code> <p>Flag indicating if runtime outputs should be verbose. Defaults to False.</p> <code>frozen</code> <code>bool</code> <p>Flag indicating if the skill is frozen. Defaults to False.</p> <code>type</code> <code>ClassVar[str]</code> <p>Type of the skill.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>class Skill(BaseModelInRegistry):\n    \"\"\"\n    Abstract base class representing a skill.\n\n    Provides methods to interact with and obtain information about skills.\n\n    Attributes:\n        name (str): Unique name of the skill.\n        instructions (str): Instructs agent what to do with the input data.\n        input_template (str): Template for the input data.\n        output_template (str): Template for the output data.\n        description (Optional[str]): Description of the skill.\n        field_schema (Optional[Dict]): Field [JSON schema](https://json-schema.org/) to use in the templates. Defaults to all fields are strings,\n            i.e. analogous to {\"field_n\": {\"type\": \"string\"}}.\n        extra_fields (Optional[Dict[str, str]]): Extra fields to use in the templates. Defaults to None.\n        instructions_first (bool): Flag indicating if instructions should be executed before input. Defaults to True.\n        verbose (bool): Flag indicating if runtime outputs should be verbose. Defaults to False.\n        frozen (bool): Flag indicating if the skill is frozen. Defaults to False.\n        type (ClassVar[str]): Type of the skill.\n    \"\"\"\n\n    name: str = Field(\n        title=\"Skill name\",\n        description=\"Unique name of the skill\",\n        examples=[\"labeling\", \"classification\", \"text-generation\"],\n    )\n    instructions: str = Field(\n        title=\"Skill instructions\",\n        description=\"Instructs agent what to do with the input data. \"\n        \"Can use templating to refer to input fields.\",\n        examples=[\"Label the input text with the following labels: {labels}\"],\n    )\n    input_template: str = Field(\n        title=\"Input template\",\n        description=\"Template for the input data. \"\n        \"Can use templating to refer to input parameters and perform data transformations.\",\n        examples=[\"Input: {input}\", \"Input: {input}\\nLabels: {labels}\\nOutput: \"],\n    )\n    output_template: str = Field(\n        title=\"Output template\",\n        description=\"Template for the output data. \"\n        \"Can use templating to refer to input parameters and perform data transformations\",\n        examples=[\"Output: {output}\", \"{predictions}\"],\n    )\n    description: Optional[str] = Field(\n        default=\"\",\n        title=\"Skill description\",\n        description=\"Description of the skill. Can be used to retrieve skill from the library.\",\n        examples=[\"The skill to perform sentiment analysis on the input text.\"],\n    )\n    field_schema: Optional[Dict[str, Any]] = Field(\n        default=None,\n        title=\"Field schema\",\n        description=\"JSON schema for the fields of the input and output data.\",\n        examples=[\n            {\n                \"input\": {\"type\": \"string\"},\n                \"output\": {\"type\": \"string\"},\n                \"labels\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"type\": \"string\",\n                        \"enum\": [\"positive\", \"negative\", \"neutral\"],\n                    },\n                },\n            }\n        ],\n    )\n    instructions_first: bool = Field(\n        default=True,\n        title=\"Instructions first\",\n        description=\"Flag indicating if instructions should be shown before the input data.\",\n        examples=[True, False],\n    )\n\n    frozen: bool = Field(\n        default=False,\n        title=\"Frozen\",\n        description=\"Flag indicating if the skill is frozen.\",\n        examples=[True, False],\n    )\n\n    def _get_extra_fields(self):\n        \"\"\"\n        Retrieves fields that are not categorized as system fields.\n\n        Returns:\n            dict: A dictionary containing fields that are not system fields.\n        \"\"\"\n\n        # TODO: more robust way to exclude system fields\n        system_fields = {\n            \"name\",\n            \"description\",\n            \"input_template\",\n            \"output_template\",\n            \"instructions\",\n            \"field_schema\",\n        }\n        extra_fields = self.model_dump(exclude=system_fields)\n        return extra_fields\n\n    def get_output_fields(self):\n        \"\"\"\n        Retrieves output fields.\n\n        Returns:\n            List[str]: A list of output fields.\n        \"\"\"\n        extra_fields = self._get_extra_fields()\n        # TODO: input fields are not considered - shall we disallow input fields in output template?\n        output_fields = parse_template(\n            partial_str_format(self.output_template, **extra_fields),\n            include_texts=False,\n        )\n        return [f[\"text\"] for f in output_fields]\n\n    @abstractmethod\n    def apply(self, input, runtime):\n        \"\"\"\n        Base method for applying the skill.\n        \"\"\"\n\n    @abstractmethod\n    def improve(self, predictions, train_skill_output, feedback, runtime):\n        \"\"\"\n        Base method for improving the skill.\n        \"\"\"\n</code></pre>"},{"location":"skills/#adala.skills._base.Skill.apply","title":"<code>apply(input, runtime)</code>  <code>abstractmethod</code>","text":"<p>Base method for applying the skill.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>@abstractmethod\ndef apply(self, input, runtime):\n    \"\"\"\n    Base method for applying the skill.\n    \"\"\"\n</code></pre>"},{"location":"skills/#adala.skills._base.Skill.get_output_fields","title":"<code>get_output_fields()</code>","text":"<p>Retrieves output fields.</p> <p>Returns:</p> Type Description <p>List[str]: A list of output fields.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>def get_output_fields(self):\n    \"\"\"\n    Retrieves output fields.\n\n    Returns:\n        List[str]: A list of output fields.\n    \"\"\"\n    extra_fields = self._get_extra_fields()\n    # TODO: input fields are not considered - shall we disallow input fields in output template?\n    output_fields = parse_template(\n        partial_str_format(self.output_template, **extra_fields),\n        include_texts=False,\n    )\n    return [f[\"text\"] for f in output_fields]\n</code></pre>"},{"location":"skills/#adala.skills._base.Skill.improve","title":"<code>improve(predictions, train_skill_output, feedback, runtime)</code>  <code>abstractmethod</code>","text":"<p>Base method for improving the skill.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>@abstractmethod\ndef improve(self, predictions, train_skill_output, feedback, runtime):\n    \"\"\"\n    Base method for improving the skill.\n    \"\"\"\n</code></pre>"},{"location":"skills/#adala.skills._base.SynthesisSkill","title":"<code>SynthesisSkill</code>","text":"<p>             Bases: <code>Skill</code></p> <p>Synthesis skill that synthesize a dataframe from a record (e.g. for dataset generation purposes). See base class Skill for more information about the attributes.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>class SynthesisSkill(Skill):\n    \"\"\"\n    Synthesis skill that synthesize a dataframe from a record (e.g. for dataset generation purposes).\n    See base class Skill for more information about the attributes.\n    \"\"\"\n\n    def apply(\n        self,\n        input: Union[Dict, InternalSeries],\n        runtime: Runtime,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Applies the skill to a record and returns a dataframe.\n\n        Args:\n            input (InternalSeries): The input data to be processed.\n            runtime (Runtime): The runtime instance to be used for processing.\n\n        Returns:\n            InternalDataFrame: The synthesized data.\n        \"\"\"\n        if isinstance(input, InternalSeries):\n            input = input.to_dict()\n        return runtime.record_to_batch(\n            input,\n            input_template=self.input_template,\n            output_template=self.output_template,\n            instructions_template=self.instructions,\n            field_schema=self.field_schema,\n            extra_fields=self._get_extra_fields(),\n            instructions_first=self.instructions_first,\n        )\n\n    def improve(self, **kwargs):\n        \"\"\"\n        Improves the skill.\n        \"\"\"\n        raise NotImplementedError\n</code></pre>"},{"location":"skills/#adala.skills._base.SynthesisSkill.apply","title":"<code>apply(input, runtime)</code>","text":"<p>Applies the skill to a record and returns a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>InternalSeries</code> <p>The input data to be processed.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime instance to be used for processing.</p> required <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The synthesized data.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>def apply(\n    self,\n    input: Union[Dict, InternalSeries],\n    runtime: Runtime,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Applies the skill to a record and returns a dataframe.\n\n    Args:\n        input (InternalSeries): The input data to be processed.\n        runtime (Runtime): The runtime instance to be used for processing.\n\n    Returns:\n        InternalDataFrame: The synthesized data.\n    \"\"\"\n    if isinstance(input, InternalSeries):\n        input = input.to_dict()\n    return runtime.record_to_batch(\n        input,\n        input_template=self.input_template,\n        output_template=self.output_template,\n        instructions_template=self.instructions,\n        field_schema=self.field_schema,\n        extra_fields=self._get_extra_fields(),\n        instructions_first=self.instructions_first,\n    )\n</code></pre>"},{"location":"skills/#adala.skills._base.SynthesisSkill.improve","title":"<code>improve(**kwargs)</code>","text":"<p>Improves the skill.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>def improve(self, **kwargs):\n    \"\"\"\n    Improves the skill.\n    \"\"\"\n    raise NotImplementedError\n</code></pre>"},{"location":"skills/#adala.skills._base.TransformSkill","title":"<code>TransformSkill</code>","text":"<p>             Bases: <code>Skill</code></p> <p>Transform skill that transforms a dataframe to another dataframe (e.g. for data annotation purposes). See base class Skill for more information about the attributes.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>class TransformSkill(Skill):\n    \"\"\"\n    Transform skill that transforms a dataframe to another dataframe (e.g. for data annotation purposes).\n    See base class Skill for more information about the attributes.\n    \"\"\"\n\n    def apply(\n        self,\n        input: InternalDataFrame,\n        runtime: Runtime,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Applies the skill to a dataframe and returns another dataframe.\n\n        Args:\n            input (InternalDataFrame): The input data to be processed.\n            runtime (Runtime): The runtime instance to be used for processing.\n\n        Returns:\n            InternalDataFrame: The transformed data.\n        \"\"\"\n\n        return runtime.batch_to_batch(\n            input,\n            input_template=self.input_template,\n            output_template=self.output_template,\n            instructions_template=self.instructions,\n            field_schema=self.field_schema,\n            extra_fields=self._get_extra_fields(),\n            instructions_first=self.instructions_first,\n        )\n\n    async def aapply(\n        self,\n        input: InternalDataFrame,\n        runtime: AsyncRuntime,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Applies the skill to a dataframe and returns another dataframe.\n\n        Args:\n            input (InternalDataFrame): The input data to be processed.\n            runtime (Runtime): The runtime instance to be used for processing.\n\n        Returns:\n            InternalDataFrame: The transformed data.\n        \"\"\"\n\n        return await runtime.batch_to_batch(\n            input,\n            input_template=self.input_template,\n            output_template=self.output_template,\n            instructions_template=self.instructions,\n            field_schema=self.field_schema,\n            extra_fields=self._get_extra_fields(),\n            instructions_first=self.instructions_first,\n        )\n\n    def improve(\n        self,\n        predictions: InternalDataFrame,\n        train_skill_output: str,\n        feedback,\n        runtime: Runtime,\n        add_cot: bool = False,\n    ):\n        \"\"\"\n        Improves the skill.\n\n        Args:\n            predictions (InternalDataFrame): The predictions made by the skill.\n            train_skill_output (str): The name of the output field of the skill.\n            feedback (InternalDataFrame): The feedback provided by the user.\n            runtime (Runtime): The runtime instance to be used for processing (CURRENTLY SUPPORTS ONLY `OpenAIChatRuntime`).\n            add_cot (bool): Flag indicating if the skill should be used the Chain-of-Thought strategy. Defaults to False.\n        \"\"\"\n        if feedback.feedback[train_skill_output].isna().all():\n            # No feedback left - nothing to improve\n            return\n\n        if feedback.match[train_skill_output].all():\n            # all feedback is \"correct\" - nothing to improve\n            return\n\n        fb = feedback.feedback.rename(\n            columns=lambda x: x + \"__fb\" if x in predictions.columns else x\n        )\n        analyzed_df = fb.merge(predictions, left_index=True, right_index=True)\n\n        examples = []\n\n        for i, row in enumerate(analyzed_df.to_dict(orient=\"records\")):\n            # if fb marked as NaN, skip\n            if not row[f\"{train_skill_output}__fb\"]:\n                continue\n            examples.append(\n                f\"### Example #{i}\\n\\n\"\n                f\"{self.input_template.format(**row)}\\n\\n\"\n                f\"{self.output_template.format(**row)}\\n\\n\"\n                f'User feedback: {row[f\"{train_skill_output}__fb\"]}\\n\\n'\n            )\n\n        examples = \"\\n\".join(examples)\n\n        messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n\n        # full template\n        if self.instructions_first:\n            full_template = f\"\"\"\n{{prompt}}\n{self.input_template}\n{self.output_template}\"\"\"\n        else:\n            full_template = f\"\"\"\n{self.input_template}\n{{prompt}}\n{self.output_template}\"\"\"\n\n        messages += [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\nA prompt is a text paragraph that outlines the expected actions and instructs the large language model (LLM) to \\\ngenerate a specific output. This prompt is concatenated with the input text, and the \\\nmodel then creates the required output.\nThis describes the full template how the prompt is concatenated with the input to produce the output:\n\n```\n{full_template}\n```\n\nHere:\n- \"{self.input_template}\" is input template,\n- \"{{prompt}}\" is the LLM prompt,\n- \"{self.output_template}\" is the output template.\n\nModel can produce erroneous output if a prompt is not well defined. \\\nIn our collaboration, we\u2019ll work together to refine a prompt. The process consists of two main steps:\n\n## Step 1\nI will provide you with the current prompt along with prediction examples. Each example contains the input text, the final prediction produced by the model, and the user feedback. \\\nUser feedback indicates whether the model prediction is correct or not. \\\nYour task is to analyze the examples and user feedback, determining whether the \\\nexisting instruction is describing the task reflected by these examples precisely, and suggests changes to the prompt to address the incorrect predictions.\n\n## Step 2\nNext, you will carefully review your reasoning in step 1, integrate the insights to refine the prompt, \\\nand provide me with the new prompt that improves the model\u2019s performance.\"\"\",\n            }\n        ]\n\n        messages += [\n            {\n                \"role\": \"assistant\",\n                \"content\": \"Sure, I\u2019d be happy to help you with this prompt engineering problem. \"\n                \"Please provide me with the current prompt and the examples with user feedback.\",\n            }\n        ]\n\n        messages += [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n## Current prompt\n{self.instructions}\n\n## Examples\n{examples}\n\nSummarize your analysis about incorrect predictions and suggest changes to the prompt.\"\"\",\n            }\n        ]\n        reasoning = runtime.execute(messages)\n\n        messages += [\n            {\"role\": \"assistant\", \"content\": reasoning},\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\nNow please carefully review your reasoning in Step 1 and help with Step 2: refining the prompt.\n\n## Current prompt\n{self.instructions}\n\n## Follow this guidance to refine the prompt:\n\n1. The new prompt should should describe the task precisely, and address the points raised in the user feedback.\n\n2. The new prompt should be similar to the current prompt, and only differ in the parts that address the issues you identified in Step 1.\n    Example:\n    - Current prompt: \"Generate a summary of the input text.\"\n    - New prompt: \"Generate a summary of the input text. Pay attention to the original style.\"\n\n3. Reply only with the new prompt. Do not include input and output templates in the prompt.\n\"\"\",\n            },\n        ]\n\n        if add_cot:\n            cot_instructions = \"\"\"\n\n4. In the new prompt, you should ask the model to perform step-by-step reasoning, and provide rationale or explanations for its prediction before giving the final answer. \\\nInstruct the model to give the final answer at the end of the prompt, using the following template: \"Final answer: &lt;answer&gt;\".\n    Example:\n    - Current prompt: \"Generate a summary of the input text.\"\n    - New prompt: \"Generate a summary of the input text. Explain your reasoning step-by-step. Use the following template to give the final answer at the end of the prompt: \"Final answer: &lt;answer&gt;\".\"\"\"\n            messages[-1][\"content\"] += cot_instructions\n        # display dialogue:\n        for message in messages:\n            print(f'\"{{{message[\"role\"]}}}\":\\n{message[\"content\"]}')\n        new_prompt = runtime.execute(messages)\n        self.instructions = new_prompt\n</code></pre>"},{"location":"skills/#adala.skills._base.TransformSkill.aapply","title":"<code>aapply(input, runtime)</code>  <code>async</code>","text":"<p>Applies the skill to a dataframe and returns another dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>InternalDataFrame</code> <p>The input data to be processed.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime instance to be used for processing.</p> required <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The transformed data.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>async def aapply(\n    self,\n    input: InternalDataFrame,\n    runtime: AsyncRuntime,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Applies the skill to a dataframe and returns another dataframe.\n\n    Args:\n        input (InternalDataFrame): The input data to be processed.\n        runtime (Runtime): The runtime instance to be used for processing.\n\n    Returns:\n        InternalDataFrame: The transformed data.\n    \"\"\"\n\n    return await runtime.batch_to_batch(\n        input,\n        input_template=self.input_template,\n        output_template=self.output_template,\n        instructions_template=self.instructions,\n        field_schema=self.field_schema,\n        extra_fields=self._get_extra_fields(),\n        instructions_first=self.instructions_first,\n    )\n</code></pre>"},{"location":"skills/#adala.skills._base.TransformSkill.apply","title":"<code>apply(input, runtime)</code>","text":"<p>Applies the skill to a dataframe and returns another dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>InternalDataFrame</code> <p>The input data to be processed.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime instance to be used for processing.</p> required <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The transformed data.</p> Source code in <code>adala/skills/_base.py</code> <pre><code>def apply(\n    self,\n    input: InternalDataFrame,\n    runtime: Runtime,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Applies the skill to a dataframe and returns another dataframe.\n\n    Args:\n        input (InternalDataFrame): The input data to be processed.\n        runtime (Runtime): The runtime instance to be used for processing.\n\n    Returns:\n        InternalDataFrame: The transformed data.\n    \"\"\"\n\n    return runtime.batch_to_batch(\n        input,\n        input_template=self.input_template,\n        output_template=self.output_template,\n        instructions_template=self.instructions,\n        field_schema=self.field_schema,\n        extra_fields=self._get_extra_fields(),\n        instructions_first=self.instructions_first,\n    )\n</code></pre>"},{"location":"skills/#adala.skills._base.TransformSkill.improve","title":"<code>improve(predictions, train_skill_output, feedback, runtime, add_cot=False)</code>","text":"<p>Improves the skill.</p> <p>Parameters:</p> Name Type Description Default <code>predictions</code> <code>InternalDataFrame</code> <p>The predictions made by the skill.</p> required <code>train_skill_output</code> <code>str</code> <p>The name of the output field of the skill.</p> required <code>feedback</code> <code>InternalDataFrame</code> <p>The feedback provided by the user.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime instance to be used for processing (CURRENTLY SUPPORTS ONLY <code>OpenAIChatRuntime</code>).</p> required <code>add_cot</code> <code>bool</code> <p>Flag indicating if the skill should be used the Chain-of-Thought strategy. Defaults to False.</p> <code>False</code> Source code in <code>adala/skills/_base.py</code> <pre><code>    def improve(\n        self,\n        predictions: InternalDataFrame,\n        train_skill_output: str,\n        feedback,\n        runtime: Runtime,\n        add_cot: bool = False,\n    ):\n        \"\"\"\n        Improves the skill.\n\n        Args:\n            predictions (InternalDataFrame): The predictions made by the skill.\n            train_skill_output (str): The name of the output field of the skill.\n            feedback (InternalDataFrame): The feedback provided by the user.\n            runtime (Runtime): The runtime instance to be used for processing (CURRENTLY SUPPORTS ONLY `OpenAIChatRuntime`).\n            add_cot (bool): Flag indicating if the skill should be used the Chain-of-Thought strategy. Defaults to False.\n        \"\"\"\n        if feedback.feedback[train_skill_output].isna().all():\n            # No feedback left - nothing to improve\n            return\n\n        if feedback.match[train_skill_output].all():\n            # all feedback is \"correct\" - nothing to improve\n            return\n\n        fb = feedback.feedback.rename(\n            columns=lambda x: x + \"__fb\" if x in predictions.columns else x\n        )\n        analyzed_df = fb.merge(predictions, left_index=True, right_index=True)\n\n        examples = []\n\n        for i, row in enumerate(analyzed_df.to_dict(orient=\"records\")):\n            # if fb marked as NaN, skip\n            if not row[f\"{train_skill_output}__fb\"]:\n                continue\n            examples.append(\n                f\"### Example #{i}\\n\\n\"\n                f\"{self.input_template.format(**row)}\\n\\n\"\n                f\"{self.output_template.format(**row)}\\n\\n\"\n                f'User feedback: {row[f\"{train_skill_output}__fb\"]}\\n\\n'\n            )\n\n        examples = \"\\n\".join(examples)\n\n        messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n\n        # full template\n        if self.instructions_first:\n            full_template = f\"\"\"\n{{prompt}}\n{self.input_template}\n{self.output_template}\"\"\"\n        else:\n            full_template = f\"\"\"\n{self.input_template}\n{{prompt}}\n{self.output_template}\"\"\"\n\n        messages += [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\nA prompt is a text paragraph that outlines the expected actions and instructs the large language model (LLM) to \\\ngenerate a specific output. This prompt is concatenated with the input text, and the \\\nmodel then creates the required output.\nThis describes the full template how the prompt is concatenated with the input to produce the output:\n\n```\n{full_template}\n```\n\nHere:\n- \"{self.input_template}\" is input template,\n- \"{{prompt}}\" is the LLM prompt,\n- \"{self.output_template}\" is the output template.\n\nModel can produce erroneous output if a prompt is not well defined. \\\nIn our collaboration, we\u2019ll work together to refine a prompt. The process consists of two main steps:\n\n## Step 1\nI will provide you with the current prompt along with prediction examples. Each example contains the input text, the final prediction produced by the model, and the user feedback. \\\nUser feedback indicates whether the model prediction is correct or not. \\\nYour task is to analyze the examples and user feedback, determining whether the \\\nexisting instruction is describing the task reflected by these examples precisely, and suggests changes to the prompt to address the incorrect predictions.\n\n## Step 2\nNext, you will carefully review your reasoning in step 1, integrate the insights to refine the prompt, \\\nand provide me with the new prompt that improves the model\u2019s performance.\"\"\",\n            }\n        ]\n\n        messages += [\n            {\n                \"role\": \"assistant\",\n                \"content\": \"Sure, I\u2019d be happy to help you with this prompt engineering problem. \"\n                \"Please provide me with the current prompt and the examples with user feedback.\",\n            }\n        ]\n\n        messages += [\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\n## Current prompt\n{self.instructions}\n\n## Examples\n{examples}\n\nSummarize your analysis about incorrect predictions and suggest changes to the prompt.\"\"\",\n            }\n        ]\n        reasoning = runtime.execute(messages)\n\n        messages += [\n            {\"role\": \"assistant\", \"content\": reasoning},\n            {\n                \"role\": \"user\",\n                \"content\": f\"\"\"\nNow please carefully review your reasoning in Step 1 and help with Step 2: refining the prompt.\n\n## Current prompt\n{self.instructions}\n\n## Follow this guidance to refine the prompt:\n\n1. The new prompt should should describe the task precisely, and address the points raised in the user feedback.\n\n2. The new prompt should be similar to the current prompt, and only differ in the parts that address the issues you identified in Step 1.\n    Example:\n    - Current prompt: \"Generate a summary of the input text.\"\n    - New prompt: \"Generate a summary of the input text. Pay attention to the original style.\"\n\n3. Reply only with the new prompt. Do not include input and output templates in the prompt.\n\"\"\",\n            },\n        ]\n\n        if add_cot:\n            cot_instructions = \"\"\"\n\n4. In the new prompt, you should ask the model to perform step-by-step reasoning, and provide rationale or explanations for its prediction before giving the final answer. \\\nInstruct the model to give the final answer at the end of the prompt, using the following template: \"Final answer: &lt;answer&gt;\".\n    Example:\n    - Current prompt: \"Generate a summary of the input text.\"\n    - New prompt: \"Generate a summary of the input text. Explain your reasoning step-by-step. Use the following template to give the final answer at the end of the prompt: \"Final answer: &lt;answer&gt;\".\"\"\"\n            messages[-1][\"content\"] += cot_instructions\n        # display dialogue:\n        for message in messages:\n            print(f'\"{{{message[\"role\"]}}}\":\\n{message[\"content\"]}')\n        new_prompt = runtime.execute(messages)\n        self.instructions = new_prompt\n</code></pre>"},{"location":"skills/#adala.skills.skillset.LinearSkillSet","title":"<code>LinearSkillSet</code>","text":"<p>             Bases: <code>SkillSet</code></p> <p>Represents a sequence of skills that are acquired in a specific order to achieve a goal.</p> <p>LinearSkillSet ensures that skills are applied in a sequential manner.</p> <p>Attributes:</p> Name Type Description <code>skills</code> <code>Union[List[Skill], Dict[str, Skill]]</code> <p>Provided skills</p> <code>skill_sequence</code> <code>List[str]</code> <p>Ordered list of skill names indicating the order                                   in which they should be acquired.</p> <p>Examples:</p> <pre><code>Create a LinearSkillSet with a list of skills specified as BaseSkill instances:\n&gt;&gt;&gt; from adala.skills import LinearSkillSet, TransformSkill, AnalysisSkill, ClassificationSkill\n&gt;&gt;&gt; skillset = LinearSkillSet(skills=[TransformSkill(), ClassificationSkill(), AnalysisSkill()])\n</code></pre> Source code in <code>adala/skills/skillset.py</code> <pre><code>class LinearSkillSet(SkillSet):\n    \"\"\"\n    Represents a sequence of skills that are acquired in a specific order to achieve a goal.\n\n    LinearSkillSet ensures that skills are applied in a sequential manner.\n\n    Attributes:\n        skills (Union[List[Skill], Dict[str, Skill]]): Provided skills\n        skill_sequence (List[str], optional): Ordered list of skill names indicating the order\n                                              in which they should be acquired.\n\n    Examples:\n\n        Create a LinearSkillSet with a list of skills specified as BaseSkill instances:\n        &gt;&gt;&gt; from adala.skills import LinearSkillSet, TransformSkill, AnalysisSkill, ClassificationSkill\n        &gt;&gt;&gt; skillset = LinearSkillSet(skills=[TransformSkill(), ClassificationSkill(), AnalysisSkill()])\n    \"\"\"\n\n    skill_sequence: List[str] = None\n\n    @model_validator(mode=\"after\")\n    def skill_sequence_validator(self) -&gt; \"LinearSkillSet\":\n        \"\"\"\n        Validates and sets the default order for the skill sequence if not provided.\n\n        Returns:\n            LinearSkillSet: The current instance with updated skill_sequence attribute.\n        \"\"\"\n        if self.skill_sequence is None:\n            # use default skill sequence defined by lexicographical order\n            self.skill_sequence = list(self.skills.keys())\n        if len(self.skill_sequence) != len(self.skills):\n            raise ValueError(\n                f\"skill_sequence must contain all skill names - \"\n                f\"length of skill_sequence is {len(self.skill_sequence)} \"\n                f\"while length of skills is {len(self.skills)}\"\n            )\n        return self\n\n    def apply(\n        self,\n        input: Union[Record, InternalDataFrame],\n        runtime: Runtime,\n        improved_skill: Optional[str] = None,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Sequentially applies each skill on the dataset.\n\n        Args:\n            input (InternalDataFrame): Input dataset.\n            runtime (Runtime): The runtime environment in which to apply the skills.\n            improved_skill (Optional[str], optional): Name of the skill to improve. Defaults to None.\n        Returns:\n            InternalDataFrame: Skill predictions.\n        \"\"\"\n        if improved_skill:\n            # start from the specified skill, assuming previous skills have already been applied\n            skill_sequence = self.skill_sequence[\n                self.skill_sequence.index(improved_skill) :\n            ]\n        else:\n            skill_sequence = self.skill_sequence\n        skill_input = input\n        for i, skill_name in enumerate(skill_sequence):\n            skill = self.skills[skill_name]\n            # use input dataset for the first node in the pipeline\n            print_text(f\"Applying skill: {skill_name}\")\n            skill_output = skill.apply(skill_input, runtime)\n            print_dataframe(skill_output)\n            if isinstance(skill, TransformSkill):\n                # Columns to drop from skill_input because they are also in skill_output\n                cols_to_drop = set(skill_output.columns) &amp; set(skill_input.columns)\n                skill_input_reduced = skill_input.drop(columns=cols_to_drop)\n\n                skill_input = skill_input_reduced.merge(\n                    skill_output, left_index=True, right_index=True, how=\"inner\"\n                )\n            elif isinstance(skill, (AnalysisSkill, SynthesisSkill)):\n                skill_input = skill_output\n            else:\n                raise ValueError(f\"Unsupported skill type: {type(skill)}\")\n        if isinstance(skill_input, InternalSeries):\n            skill_input = skill_input.to_frame().T\n        return skill_input\n\n    async def aapply(\n        self,\n        input: Union[Record, InternalDataFrame],\n        runtime: AsyncRuntime,\n        improved_skill: Optional[str] = None,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Sequentially and asynchronously applies each skill on the dataset.\n\n        Args:\n            input (InternalDataFrame): Input dataset.\n            runtime (AsyncRuntime): The runtime environment in which to apply the skills.\n            improved_skill (Optional[str], optional): Name of the skill to improve. Defaults to None.\n        Returns:\n            InternalDataFrame: Skill predictions.\n        \"\"\"\n        if improved_skill:\n            # start from the specified skill, assuming previous skills have already been applied\n            skill_sequence = self.skill_sequence[\n                self.skill_sequence.index(improved_skill) :\n            ]\n        else:\n            skill_sequence = self.skill_sequence\n        skill_input = input\n        for i, skill_name in enumerate(skill_sequence):\n            skill = self.skills[skill_name]\n            # use input dataset for the first node in the pipeline\n            print_text(f\"Applying skill: {skill_name}\")\n            skill_output = await skill.aapply(skill_input, runtime)\n            print_dataframe(skill_output)\n            if isinstance(skill, TransformSkill):\n                # Columns to drop from skill_input because they are also in skill_output\n                cols_to_drop = set(skill_output.columns) &amp; set(skill_input.columns)\n                skill_input_reduced = skill_input.drop(columns=cols_to_drop)\n\n                skill_input = skill_input_reduced.merge(\n                    skill_output, left_index=True, right_index=True, how=\"inner\"\n                )\n            elif isinstance(skill, (AnalysisSkill, SynthesisSkill)):\n                skill_input = skill_output\n            else:\n                raise ValueError(f\"Unsupported skill type: {type(skill)}\")\n        if isinstance(skill_input, InternalSeries):\n            skill_input = skill_input.to_frame().T\n        return skill_input\n\n    def __rich__(self):\n        \"\"\"Returns a rich representation of the skill.\"\"\"\n        # TODO: move it to a base class and use repr derived from Skills\n        text = f\"[bold blue]Total Agent Skills: {len(self.skills)}[/bold blue]\\n\\n\"\n        for skill in self.skills.values():\n            text += (\n                f\"[bold underline green]{skill.name}[/bold underline green]\\n\"\n                f\"[green]{skill.instructions}[green]\\n\"\n            )\n        return text\n</code></pre>"},{"location":"skills/#adala.skills.skillset.LinearSkillSet.__rich__","title":"<code>__rich__()</code>","text":"<p>Returns a rich representation of the skill.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>def __rich__(self):\n    \"\"\"Returns a rich representation of the skill.\"\"\"\n    # TODO: move it to a base class and use repr derived from Skills\n    text = f\"[bold blue]Total Agent Skills: {len(self.skills)}[/bold blue]\\n\\n\"\n    for skill in self.skills.values():\n        text += (\n            f\"[bold underline green]{skill.name}[/bold underline green]\\n\"\n            f\"[green]{skill.instructions}[green]\\n\"\n        )\n    return text\n</code></pre>"},{"location":"skills/#adala.skills.skillset.LinearSkillSet.aapply","title":"<code>aapply(input, runtime, improved_skill=None)</code>  <code>async</code>","text":"<p>Sequentially and asynchronously applies each skill on the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>InternalDataFrame</code> <p>Input dataset.</p> required <code>runtime</code> <code>AsyncRuntime</code> <p>The runtime environment in which to apply the skills.</p> required <code>improved_skill</code> <code>Optional[str]</code> <p>Name of the skill to improve. Defaults to None.</p> <code>None</code> <p>Returns:     InternalDataFrame: Skill predictions.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>async def aapply(\n    self,\n    input: Union[Record, InternalDataFrame],\n    runtime: AsyncRuntime,\n    improved_skill: Optional[str] = None,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Sequentially and asynchronously applies each skill on the dataset.\n\n    Args:\n        input (InternalDataFrame): Input dataset.\n        runtime (AsyncRuntime): The runtime environment in which to apply the skills.\n        improved_skill (Optional[str], optional): Name of the skill to improve. Defaults to None.\n    Returns:\n        InternalDataFrame: Skill predictions.\n    \"\"\"\n    if improved_skill:\n        # start from the specified skill, assuming previous skills have already been applied\n        skill_sequence = self.skill_sequence[\n            self.skill_sequence.index(improved_skill) :\n        ]\n    else:\n        skill_sequence = self.skill_sequence\n    skill_input = input\n    for i, skill_name in enumerate(skill_sequence):\n        skill = self.skills[skill_name]\n        # use input dataset for the first node in the pipeline\n        print_text(f\"Applying skill: {skill_name}\")\n        skill_output = await skill.aapply(skill_input, runtime)\n        print_dataframe(skill_output)\n        if isinstance(skill, TransformSkill):\n            # Columns to drop from skill_input because they are also in skill_output\n            cols_to_drop = set(skill_output.columns) &amp; set(skill_input.columns)\n            skill_input_reduced = skill_input.drop(columns=cols_to_drop)\n\n            skill_input = skill_input_reduced.merge(\n                skill_output, left_index=True, right_index=True, how=\"inner\"\n            )\n        elif isinstance(skill, (AnalysisSkill, SynthesisSkill)):\n            skill_input = skill_output\n        else:\n            raise ValueError(f\"Unsupported skill type: {type(skill)}\")\n    if isinstance(skill_input, InternalSeries):\n        skill_input = skill_input.to_frame().T\n    return skill_input\n</code></pre>"},{"location":"skills/#adala.skills.skillset.LinearSkillSet.apply","title":"<code>apply(input, runtime, improved_skill=None)</code>","text":"<p>Sequentially applies each skill on the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>InternalDataFrame</code> <p>Input dataset.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime environment in which to apply the skills.</p> required <code>improved_skill</code> <code>Optional[str]</code> <p>Name of the skill to improve. Defaults to None.</p> <code>None</code> <p>Returns:     InternalDataFrame: Skill predictions.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>def apply(\n    self,\n    input: Union[Record, InternalDataFrame],\n    runtime: Runtime,\n    improved_skill: Optional[str] = None,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Sequentially applies each skill on the dataset.\n\n    Args:\n        input (InternalDataFrame): Input dataset.\n        runtime (Runtime): The runtime environment in which to apply the skills.\n        improved_skill (Optional[str], optional): Name of the skill to improve. Defaults to None.\n    Returns:\n        InternalDataFrame: Skill predictions.\n    \"\"\"\n    if improved_skill:\n        # start from the specified skill, assuming previous skills have already been applied\n        skill_sequence = self.skill_sequence[\n            self.skill_sequence.index(improved_skill) :\n        ]\n    else:\n        skill_sequence = self.skill_sequence\n    skill_input = input\n    for i, skill_name in enumerate(skill_sequence):\n        skill = self.skills[skill_name]\n        # use input dataset for the first node in the pipeline\n        print_text(f\"Applying skill: {skill_name}\")\n        skill_output = skill.apply(skill_input, runtime)\n        print_dataframe(skill_output)\n        if isinstance(skill, TransformSkill):\n            # Columns to drop from skill_input because they are also in skill_output\n            cols_to_drop = set(skill_output.columns) &amp; set(skill_input.columns)\n            skill_input_reduced = skill_input.drop(columns=cols_to_drop)\n\n            skill_input = skill_input_reduced.merge(\n                skill_output, left_index=True, right_index=True, how=\"inner\"\n            )\n        elif isinstance(skill, (AnalysisSkill, SynthesisSkill)):\n            skill_input = skill_output\n        else:\n            raise ValueError(f\"Unsupported skill type: {type(skill)}\")\n    if isinstance(skill_input, InternalSeries):\n        skill_input = skill_input.to_frame().T\n    return skill_input\n</code></pre>"},{"location":"skills/#adala.skills.skillset.LinearSkillSet.skill_sequence_validator","title":"<code>skill_sequence_validator()</code>","text":"<p>Validates and sets the default order for the skill sequence if not provided.</p> <p>Returns:</p> Name Type Description <code>LinearSkillSet</code> <code>LinearSkillSet</code> <p>The current instance with updated skill_sequence attribute.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>@model_validator(mode=\"after\")\ndef skill_sequence_validator(self) -&gt; \"LinearSkillSet\":\n    \"\"\"\n    Validates and sets the default order for the skill sequence if not provided.\n\n    Returns:\n        LinearSkillSet: The current instance with updated skill_sequence attribute.\n    \"\"\"\n    if self.skill_sequence is None:\n        # use default skill sequence defined by lexicographical order\n        self.skill_sequence = list(self.skills.keys())\n    if len(self.skill_sequence) != len(self.skills):\n        raise ValueError(\n            f\"skill_sequence must contain all skill names - \"\n            f\"length of skill_sequence is {len(self.skill_sequence)} \"\n            f\"while length of skills is {len(self.skills)}\"\n        )\n    return self\n</code></pre>"},{"location":"skills/#adala.skills.skillset.ParallelSkillSet","title":"<code>ParallelSkillSet</code>","text":"<p>             Bases: <code>SkillSet</code></p> <p>Represents a set of skills that are acquired simultaneously to reach a goal.</p> <p>In a ParallelSkillSet, each skill can be developed independently of the others. This is useful for agents that require multiple, diverse capabilities, or tasks where each skill contributes a piece of the overall solution.</p> <p>Examples:</p> <p>Create a ParallelSkillSet with a list of skills specified as BaseSkill instances</p> <pre><code>&gt;&gt;&gt; from adala.skills import ParallelSkillSet, ClassificationSkill, TransformSkill\n&gt;&gt;&gt; skillset = ParallelSkillSet(skills=[ClassificationSkill(), TransformSkill()])\n</code></pre> Source code in <code>adala/skills/skillset.py</code> <pre><code>class ParallelSkillSet(SkillSet):\n    \"\"\"\n    Represents a set of skills that are acquired simultaneously to reach a goal.\n\n    In a ParallelSkillSet, each skill can be developed independently of the others. This is useful\n    for agents that require multiple, diverse capabilities, or tasks where each skill contributes a piece of\n    the overall solution.\n\n    Examples:\n        Create a ParallelSkillSet with a list of skills specified as BaseSkill instances\n        &gt;&gt;&gt; from adala.skills import ParallelSkillSet, ClassificationSkill, TransformSkill\n        &gt;&gt;&gt; skillset = ParallelSkillSet(skills=[ClassificationSkill(), TransformSkill()])\n    \"\"\"\n\n    def apply(\n        self,\n        input: Union[InternalSeries, InternalDataFrame],\n        runtime: Runtime,\n        improved_skill: Optional[str] = None,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Applies each skill on the dataset, enhancing the agent's experience.\n\n        Args:\n            input (Union[Record, InternalDataFrame]): Input data\n            runtime (Runtime): The runtime environment in which to apply the skills.\n            improved_skill (Optional[str], optional): Unused in ParallelSkillSet. Defaults to None.\n        Returns:\n            Union[Record, InternalDataFrame]: Skill predictions.\n        \"\"\"\n        if improved_skill:\n            # start from the specified skill, assuming previous skills have already been applied\n            skill_sequence = [improved_skill]\n        else:\n            skill_sequence = list(self.skills.keys())\n\n        skill_outputs = []\n        for i, skill_name in enumerate(skill_sequence):\n            skill = self.skills[skill_name]\n            # use input dataset for the first node in the pipeline\n            print_text(f\"Applying skill: {skill_name}\")\n            skill_output = skill.apply(input, runtime)\n            skill_outputs.append(skill_output)\n        if not skill_outputs:\n            return InternalDataFrame()\n        else:\n            if isinstance(skill_outputs[0], InternalDataFrame):\n                skill_outputs = InternalDataFrameConcat(skill_outputs, axis=1)\n                cols_to_drop = set(input.columns) &amp; set(skill_outputs.columns)\n                skill_input_reduced = input.drop(columns=cols_to_drop)\n\n                return skill_input_reduced.merge(\n                    skill_outputs, left_index=True, right_index=True, how=\"inner\"\n                )\n            elif isinstance(skill_outputs[0], (dict, InternalSeries)):\n                # concatenate output to each row of input\n                output = skill_outputs[0]\n                return InternalDataFrameConcat(\n                    [\n                        input,\n                        InternalDataFrame(\n                            [output] * len(input),\n                            columns=output.index,\n                            index=input.index,\n                        ),\n                    ],\n                    axis=1,\n                )\n            else:\n                raise ValueError(f\"Unsupported output type: {type(skill_outputs[0])}\")\n</code></pre>"},{"location":"skills/#adala.skills.skillset.ParallelSkillSet.apply","title":"<code>apply(input, runtime, improved_skill=None)</code>","text":"<p>Applies each skill on the dataset, enhancing the agent's experience.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Union[Record, InternalDataFrame]</code> <p>Input data</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime environment in which to apply the skills.</p> required <code>improved_skill</code> <code>Optional[str]</code> <p>Unused in ParallelSkillSet. Defaults to None.</p> <code>None</code> <p>Returns:     Union[Record, InternalDataFrame]: Skill predictions.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>def apply(\n    self,\n    input: Union[InternalSeries, InternalDataFrame],\n    runtime: Runtime,\n    improved_skill: Optional[str] = None,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Applies each skill on the dataset, enhancing the agent's experience.\n\n    Args:\n        input (Union[Record, InternalDataFrame]): Input data\n        runtime (Runtime): The runtime environment in which to apply the skills.\n        improved_skill (Optional[str], optional): Unused in ParallelSkillSet. Defaults to None.\n    Returns:\n        Union[Record, InternalDataFrame]: Skill predictions.\n    \"\"\"\n    if improved_skill:\n        # start from the specified skill, assuming previous skills have already been applied\n        skill_sequence = [improved_skill]\n    else:\n        skill_sequence = list(self.skills.keys())\n\n    skill_outputs = []\n    for i, skill_name in enumerate(skill_sequence):\n        skill = self.skills[skill_name]\n        # use input dataset for the first node in the pipeline\n        print_text(f\"Applying skill: {skill_name}\")\n        skill_output = skill.apply(input, runtime)\n        skill_outputs.append(skill_output)\n    if not skill_outputs:\n        return InternalDataFrame()\n    else:\n        if isinstance(skill_outputs[0], InternalDataFrame):\n            skill_outputs = InternalDataFrameConcat(skill_outputs, axis=1)\n            cols_to_drop = set(input.columns) &amp; set(skill_outputs.columns)\n            skill_input_reduced = input.drop(columns=cols_to_drop)\n\n            return skill_input_reduced.merge(\n                skill_outputs, left_index=True, right_index=True, how=\"inner\"\n            )\n        elif isinstance(skill_outputs[0], (dict, InternalSeries)):\n            # concatenate output to each row of input\n            output = skill_outputs[0]\n            return InternalDataFrameConcat(\n                [\n                    input,\n                    InternalDataFrame(\n                        [output] * len(input),\n                        columns=output.index,\n                        index=input.index,\n                    ),\n                ],\n                axis=1,\n            )\n        else:\n            raise ValueError(f\"Unsupported output type: {type(skill_outputs[0])}\")\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet","title":"<code>SkillSet</code>","text":"<p>             Bases: <code>BaseModel</code>, <code>ABC</code></p> <p>Represents a collection of interdependent skills aiming to achieve a specific goal.</p> <p>A skill set breaks down the path to achieve a goal into necessary precursor skills. Agents can evolve these skills either in parallel for tasks like self-consistency or sequentially for complex problem decompositions and causal reasoning. In the most generic cases, task decomposition can involve a graph-based approach.</p> <p>Attributes:</p> Name Type Description <code>skills</code> <code>Dict[str, Skill]</code> <p>A dictionary of skills in the skill set.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>class SkillSet(BaseModel, ABC):\n    \"\"\"\n    Represents a collection of interdependent skills aiming to achieve a specific goal.\n\n    A skill set breaks down the path to achieve a goal into necessary precursor skills.\n    Agents can evolve these skills either in parallel for tasks like self-consistency or\n    sequentially for complex problem decompositions and causal reasoning. In the most generic\n    cases, task decomposition can involve a graph-based approach.\n\n    Attributes:\n        skills (Dict[str, Skill]): A dictionary of skills in the skill set.\n    \"\"\"\n\n    skills: Union[List, Dict[str, Skill]]\n\n    @field_validator(\"skills\", mode=\"before\")\n    def skills_validator(cls, v: Union[List, Dict]) -&gt; Dict[str, Skill]:\n        \"\"\"\n        Validates and converts the skills attribute to a dictionary of skill names to BaseSkill instances.\n\n        Args:\n            v (Union[List[Skill], Dict[str, Skill]]): The skills attribute to validate and convert.\n\n        Returns:\n            Dict[str, BaseSkill]: Dictionary mapping skill names to their corresponding BaseSkill instances.\n        \"\"\"\n        skills = OrderedDict()\n        if not v:\n            return skills\n\n        elif isinstance(v, list):\n            if isinstance(v[0], Skill):\n                # convert list of skill names to dictionary\n                for skill in v:\n                    skills[skill.name] = skill\n            elif isinstance(v[0], dict):\n                # convert list of skill dictionaries to dictionary\n                for skill in v:\n                    if \"type\" not in skill:\n                        raise ValueError(\"Skill dictionary must contain a 'type' key\")\n                    skills[skill[\"name\"]] = Skill.create_from_registry(\n                        skill.pop(\"type\"), **skill\n                    )\n        elif isinstance(v, dict):\n            skills = v\n        else:\n            raise ValueError(\n                f\"skills must be a list or dictionary, but received type {type(v)}\"\n            )\n        return skills\n\n    @abstractmethod\n    def apply(\n        self,\n        input: Union[Record, InternalDataFrame],\n        runtime: Runtime,\n        improved_skill: Optional[str] = None,\n    ) -&gt; InternalDataFrame:\n        \"\"\"\n        Apply the skill set to a dataset using a specified runtime.\n\n        Args:\n            input (Union[Record, InternalDataFrame]): Input data to apply the skill set to.\n            runtime (Runtime): The runtime environment in which to apply the skills.\n            improved_skill (Optional[str], optional): Name of the skill to start from (to optimize calculations). Defaults to None.\n        Returns:\n            InternalDataFrame: Skill predictions.\n        \"\"\"\n\n    def __getitem__(self, skill_name) -&gt; Skill:\n        \"\"\"\n        Select skill by name.\n\n        Args:\n            skill_name (str): Name of the skill to select.\n\n        Returns:\n            BaseSkill: Skill\n        \"\"\"\n        return self.skills[skill_name]\n\n    def __setitem__(self, skill_name, skill: Skill):\n        \"\"\"\n        Set skill by name.\n\n        Args:\n            skill_name (str): Name of the skill to set.\n            skill (BaseSkill): Skill to set.\n        \"\"\"\n        self.skills[skill_name] = skill\n\n    def get_skill_names(self) -&gt; List[str]:\n        \"\"\"\n        Get list of skill names.\n\n        Returns:\n            List[str]: List of skill names.\n        \"\"\"\n        return list(self.skills.keys())\n\n    def get_skill_outputs(self) -&gt; Dict[str, str]:\n        \"\"\"\n        Get dictionary of skill outputs.\n\n        Returns:\n            Dict[str, str]: Dictionary of skill outputs. Keys are output names and values are skill names\n        \"\"\"\n        return {\n            field: skill.name\n            for skill in self.skills.values()\n            for field in skill.get_output_fields()\n        }\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet.__getitem__","title":"<code>__getitem__(skill_name)</code>","text":"<p>Select skill by name.</p> <p>Parameters:</p> Name Type Description Default <code>skill_name</code> <code>str</code> <p>Name of the skill to select.</p> required <p>Returns:</p> Name Type Description <code>BaseSkill</code> <code>Skill</code> <p>Skill</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>def __getitem__(self, skill_name) -&gt; Skill:\n    \"\"\"\n    Select skill by name.\n\n    Args:\n        skill_name (str): Name of the skill to select.\n\n    Returns:\n        BaseSkill: Skill\n    \"\"\"\n    return self.skills[skill_name]\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet.__setitem__","title":"<code>__setitem__(skill_name, skill)</code>","text":"<p>Set skill by name.</p> <p>Parameters:</p> Name Type Description Default <code>skill_name</code> <code>str</code> <p>Name of the skill to set.</p> required <code>skill</code> <code>BaseSkill</code> <p>Skill to set.</p> required Source code in <code>adala/skills/skillset.py</code> <pre><code>def __setitem__(self, skill_name, skill: Skill):\n    \"\"\"\n    Set skill by name.\n\n    Args:\n        skill_name (str): Name of the skill to set.\n        skill (BaseSkill): Skill to set.\n    \"\"\"\n    self.skills[skill_name] = skill\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet.apply","title":"<code>apply(input, runtime, improved_skill=None)</code>  <code>abstractmethod</code>","text":"<p>Apply the skill set to a dataset using a specified runtime.</p> <p>Parameters:</p> Name Type Description Default <code>input</code> <code>Union[Record, InternalDataFrame]</code> <p>Input data to apply the skill set to.</p> required <code>runtime</code> <code>Runtime</code> <p>The runtime environment in which to apply the skills.</p> required <code>improved_skill</code> <code>Optional[str]</code> <p>Name of the skill to start from (to optimize calculations). Defaults to None.</p> <code>None</code> <p>Returns:     InternalDataFrame: Skill predictions.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>@abstractmethod\ndef apply(\n    self,\n    input: Union[Record, InternalDataFrame],\n    runtime: Runtime,\n    improved_skill: Optional[str] = None,\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Apply the skill set to a dataset using a specified runtime.\n\n    Args:\n        input (Union[Record, InternalDataFrame]): Input data to apply the skill set to.\n        runtime (Runtime): The runtime environment in which to apply the skills.\n        improved_skill (Optional[str], optional): Name of the skill to start from (to optimize calculations). Defaults to None.\n    Returns:\n        InternalDataFrame: Skill predictions.\n    \"\"\"\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet.get_skill_names","title":"<code>get_skill_names()</code>","text":"<p>Get list of skill names.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of skill names.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>def get_skill_names(self) -&gt; List[str]:\n    \"\"\"\n    Get list of skill names.\n\n    Returns:\n        List[str]: List of skill names.\n    \"\"\"\n    return list(self.skills.keys())\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet.get_skill_outputs","title":"<code>get_skill_outputs()</code>","text":"<p>Get dictionary of skill outputs.</p> <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dict[str, str]: Dictionary of skill outputs. Keys are output names and values are skill names</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>def get_skill_outputs(self) -&gt; Dict[str, str]:\n    \"\"\"\n    Get dictionary of skill outputs.\n\n    Returns:\n        Dict[str, str]: Dictionary of skill outputs. Keys are output names and values are skill names\n    \"\"\"\n    return {\n        field: skill.name\n        for skill in self.skills.values()\n        for field in skill.get_output_fields()\n    }\n</code></pre>"},{"location":"skills/#adala.skills.skillset.SkillSet.skills_validator","title":"<code>skills_validator(v)</code>","text":"<p>Validates and converts the skills attribute to a dictionary of skill names to BaseSkill instances.</p> <p>Parameters:</p> Name Type Description Default <code>v</code> <code>Union[List[Skill], Dict[str, Skill]]</code> <p>The skills attribute to validate and convert.</p> required <p>Returns:</p> Type Description <code>Dict[str, Skill]</code> <p>Dict[str, BaseSkill]: Dictionary mapping skill names to their corresponding BaseSkill instances.</p> Source code in <code>adala/skills/skillset.py</code> <pre><code>@field_validator(\"skills\", mode=\"before\")\ndef skills_validator(cls, v: Union[List, Dict]) -&gt; Dict[str, Skill]:\n    \"\"\"\n    Validates and converts the skills attribute to a dictionary of skill names to BaseSkill instances.\n\n    Args:\n        v (Union[List[Skill], Dict[str, Skill]]): The skills attribute to validate and convert.\n\n    Returns:\n        Dict[str, BaseSkill]: Dictionary mapping skill names to their corresponding BaseSkill instances.\n    \"\"\"\n    skills = OrderedDict()\n    if not v:\n        return skills\n\n    elif isinstance(v, list):\n        if isinstance(v[0], Skill):\n            # convert list of skill names to dictionary\n            for skill in v:\n                skills[skill.name] = skill\n        elif isinstance(v[0], dict):\n            # convert list of skill dictionaries to dictionary\n            for skill in v:\n                if \"type\" not in skill:\n                    raise ValueError(\"Skill dictionary must contain a 'type' key\")\n                skills[skill[\"name\"]] = Skill.create_from_registry(\n                    skill.pop(\"type\"), **skill\n                )\n    elif isinstance(v, dict):\n        skills = v\n    else:\n        raise ValueError(\n            f\"skills must be a list or dictionary, but received type {type(v)}\"\n        )\n    return skills\n</code></pre>"},{"location":"utils/","title":"Utils","text":""},{"location":"utils/#adala.utils.internal_data.InternalDataFrameConcat","title":"<code>InternalDataFrameConcat(dfs, **kwargs)</code>","text":"<p>Concatenate dataframes.</p> <p>Parameters:</p> Name Type Description Default <code>dfs</code> <code>Iterable[InternalDataFrame]</code> <p>The dataframes to concatenate.</p> required <p>Returns:</p> Name Type Description <code>InternalDataFrame</code> <code>InternalDataFrame</code> <p>The concatenated dataframe.</p> Source code in <code>adala/utils/internal_data.py</code> <pre><code>def InternalDataFrameConcat(\n    dfs: Iterable[InternalDataFrame], **kwargs\n) -&gt; InternalDataFrame:\n    \"\"\"\n    Concatenate dataframes.\n\n    Args:\n        dfs (Iterable[InternalDataFrame]): The dataframes to concatenate.\n\n    Returns:\n        InternalDataFrame: The concatenated dataframe.\n    \"\"\"\n    return pd.concat(dfs, **kwargs)\n</code></pre>"},{"location":"utils/#adala.utils.logs.print_dataframe","title":"<code>print_dataframe(dataframe)</code>","text":"<p>Print dataframe to console.</p> Source code in <code>adala/utils/logs.py</code> <pre><code>def print_dataframe(dataframe: InternalDataFrame):\n    \"\"\"\n    Print dataframe to console.\n    \"\"\"\n    num_rows = 5\n    table = Table(show_header=True, header_style=\"bold magenta\")\n    # index_name = dataframe.index.name or 'index'\n    # table.add_column(index_name)\n\n    for column in dataframe.columns:\n        table.add_column(str(column))\n\n    for index, value_list in enumerate(dataframe.iloc[:num_rows].values.tolist()):\n        # row = [str(index)]\n        row = []\n        row += [str(x) for x in value_list]\n        table.add_row(*row)\n\n    # Update the style of the table\n    table.row_styles = [\"none\", \"dim\"]\n    table.box = box.SIMPLE_HEAD\n\n    console.print(table)\n</code></pre>"},{"location":"utils/#adala.utils.logs.print_error","title":"<code>print_error(text)</code>","text":"<p>Print error message to console.</p> Source code in <code>adala/utils/logs.py</code> <pre><code>def print_error(text: str):\n    \"\"\"\n    Print error message to console.\n    \"\"\"\n    error_console.print(text)\n</code></pre>"},{"location":"utils/#adala.utils.logs.print_series","title":"<code>print_series(data)</code>","text":"<p>Print series to console.</p> Source code in <code>adala/utils/logs.py</code> <pre><code>def print_series(data: InternalSeries):\n    \"\"\"\n    Print series to console.\n    \"\"\"\n\n    # Create a Rich Table with a column for each series value\n    table = Table(show_header=True, header_style=\"bold magenta\")\n\n    # Add a column for each value in the series with the index as the header\n    for index in data.index:\n        table.add_column(str(index))\n\n    # Add a single row with all the values from the series\n    table.add_row(*[str(value) for value in data])\n\n    # Print the table with the Rich console\n    console.print(table)\n</code></pre>"},{"location":"utils/#adala.utils.logs.print_text","title":"<code>print_text(text, style=None, streaming_style=False)</code>","text":"<p>Print text to console with optional style and streaming style.</p> Source code in <code>adala/utils/logs.py</code> <pre><code>def print_text(text: str, style=None, streaming_style=False):\n    \"\"\"\n    Print text to console with optional style and streaming style.\n    \"\"\"\n    if streaming_style:\n        for char in text:\n            console.print(char, sep=\"\", end=\"\", style=style)\n            time.sleep(0.01)\n        console.print()\n    else:\n        console.print(text, style=style)\n</code></pre>"},{"location":"utils/#adala.utils.matching.fuzzy_match","title":"<code>fuzzy_match(x, y, threshold=0.8)</code>","text":"<p>Fuzzy match string values in two series.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>InternalSeries</code> <p>The first series.</p> required <code>y</code> <code>InternalSeries</code> <p>The second series.</p> required <code>threshold</code> <code>float</code> <p>The threshold to use for fuzzy matching. Defaults to 0.8.</p> <code>0.8</code> <p>Returns:</p> Name Type Description <code>InternalSeries</code> <code>InternalSeries</code> <p>The series with fuzzy match results.</p> Source code in <code>adala/utils/matching.py</code> <pre><code>def fuzzy_match(x: InternalSeries, y: InternalSeries, threshold=0.8) -&gt; InternalSeries:\n    \"\"\"\n    Fuzzy match string values in two series.\n\n    Args:\n        x (InternalSeries): The first series.\n        y (InternalSeries): The second series.\n        threshold (float): The threshold to use for fuzzy matching. Defaults to 0.8.\n\n    Returns:\n        InternalSeries: The series with fuzzy match results.\n    \"\"\"\n    result = x.combine(y, lambda x, y: _fuzzy_match(x, y, threshold))\n    return result\n</code></pre>"},{"location":"utils/#adala.utils.matching.match_options","title":"<code>match_options(query, options, splitter=None)</code>","text":"<p>Match a query to a list of options. If splitter is not None, the query will be split by the splitter and each part will be matched separately, then joined by the splitter.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The query.</p> required <code>options</code> <code>List[str]</code> <p>The options.</p> required <code>splitter</code> <code>str</code> <p>The splitter. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The matched option.</p> Source code in <code>adala/utils/matching.py</code> <pre><code>def match_options(query: str, options: List[str], splitter: str = None) -&gt; str:\n    \"\"\"\n    Match a query to a list of options.\n    If splitter is not None, the query will be split by the splitter and each part will be matched separately, then joined by the splitter.\n\n    Args:\n        query (str): The query.\n        options (List[str]): The options.\n        splitter (str): The splitter. Defaults to None.\n\n    Returns:\n        str: The matched option.\n    \"\"\"\n\n    # hard constraint: the item must be in the query\n    filtered_items = [item for item in options if item in query]\n    if not filtered_items:\n        # make the best guess - find the most similar item to the query\n        filtered_items = options\n\n    # soft constraint: find the most similar item to the query\n    matched_items = []\n    # split query by self.splitter\n    if splitter:\n        qs = query.split(splitter)\n    else:\n        qs = [query]\n\n    for q in qs:\n        scores = list(\n            map(\n                lambda item: difflib.SequenceMatcher(None, q, item).ratio(),\n                filtered_items,\n            )\n        )\n        matched_items.append(filtered_items[scores.index(max(scores))])\n    if splitter:\n        return splitter.join(matched_items)\n    return matched_items[0]\n</code></pre>"}]}